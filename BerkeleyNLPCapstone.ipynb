{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berkeley NLP Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steven Johannemann, Zoe Ouyang, Chloe Zhang\n",
    "#Import all needed packages\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import string\n",
    "from string import punctuation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset and Word2vec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "df_ori = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic View of Data\n",
    "df_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of Data\n",
    "df_ori.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column for Binary Classification where 1-2 stars = negative (0) and 3-5 stars = positive (1)\n",
    "df_ori[\"attitude\"] = np.where(df_ori[\"stars\"] >= 3, 1, 0)\n",
    "#Create a new data frame that only contains the following columns\n",
    "df = df_ori[['text','attitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8324\n",
       "0    1676\n",
       "Name: attitude, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the distribution of the 'attitude' column\n",
    "df['attitude'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXaklEQVR4nO3df7BcZ33f8fcHCYwMuLZj2XUlE4mMCsgeDNbFVUOSAg61gRQ5bdwRTWIN46LgOAm0nSky04npdDRjZtqEeFKbqIRYJgmO+GmlwSRGKT86MRbXYJBl41rBIF+kWhfnh43JyJH59o99VBZpdc/K3N17r+/7NbOz53zPec4+z0izn3t+7DmpKiRJmsmz5roDkqT5z7CQJHUyLCRJnQwLSVInw0KS1GnpXHdgVM4666xatWrVXHdDkhaUu++++9tVtfzY+jM2LFatWsXk5ORcd0OSFpQk3xxU9zCUJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqdMz9hfckjSXVm35kzn53G9c/8aRbNc9C0lSJ8NCktTJsJAkdTIsJEmdRhoWSf5dkr1J7k3yoSTPTXJmkjuSPNjez+hb/9ok+5I8kOTSvvq6JHvashuSZJT9liT9oJGFRZIVwK8BE1V1AbAE2AhsAXZV1RpgV5snydq2/HzgMuDGJEva5m4CNgNr2uuyUfVbknS8UR+GWgosS7IUOBU4AGwAtrfl24HL2/QG4NaqOlxVDwH7gIuTnAucVlV3VlUBt/S1kSSNwcjCoqq+BfxXYD9wEPjbqvoz4JyqOtjWOQic3ZqsAB7u28RUq61o08fWj5Nkc5LJJJPT09OzORxJWtRGeRjqDHp7C6uBfwQ8L8kvzNRkQK1mqB9frNpWVRNVNbF8+XGPkJUkPU2jPAz108BDVTVdVX8PfAz4ceCRdmiJ9n6orT8FnNfXfiW9w1ZTbfrYuiRpTEYZFvuB9UlObVcvXQLcD+wENrV1NgG3temdwMYkpyRZTe9E9u52qOrxJOvbdq7sayNJGoOR3Ruqqu5K8hHgS8AR4MvANuD5wI4kV9ELlCva+nuT7ADua+tfU1VPtc1dDdwMLANuby9J0piM9EaCVXUdcN0x5cP09jIGrb8V2DqgPglcMOsdlCQNxV9wS5I6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSp0yifwf3iJPf0vR5L8o4kZya5I8mD7f2MvjbXJtmX5IEkl/bV1yXZ05bd0J6YJ0kak5GFRVU9UFUvr6qXA+uA7wIfB7YAu6pqDbCrzZNkLbAROB+4DLgxyZK2uZuAzfQetbqmLZckjcm4DkNdAvxlVX0T2ABsb/XtwOVtegNwa1UdrqqHgH3AxUnOBU6rqjurqoBb+tpIksZgXGGxEfhQmz6nqg4CtPezW30F8HBfm6lWW9Gmj61LksZk5GGR5DnAm4APd606oFYz1Ad91uYkk0kmp6enT66jkqQTGseexeuBL1XVI23+kXZoifZ+qNWngPP62q0EDrT6ygH141TVtqqaqKqJ5cuXz+IQJGlxG0dYvJnvH4IC2AlsatObgNv66huTnJJkNb0T2bvboarHk6xvV0Fd2ddGkjQGS0e58SSnAq8DfqmvfD2wI8lVwH7gCoCq2ptkB3AfcAS4pqqeam2uBm4GlgG3t5ckaUxGGhZV9V3gR46pPUrv6qhB628Ftg6oTwIXjKKPkqRu/oJbktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUqeRhkWS05N8JMnXktyf5J8mOTPJHUkebO9n9K1/bZJ9SR5IcmlffV2SPW3ZDe3xqpKkMRn1nsVvAZ+qqpcAFwL3A1uAXVW1BtjV5kmyFtgInA9cBtyYZEnbzk3AZnrP5V7TlkuSxmRkYZHkNOCngN8FqKonq+pvgA3A9rbaduDyNr0BuLWqDlfVQ8A+4OIk5wKnVdWdVVXALX1tJEljMMo9ixcB08DvJflykvcneR5wTlUdBGjvZ7f1VwAP97WfarUVbfrYuiRpTEYZFkuBi4CbquoVwBO0Q04nMOg8RM1QP34DyeYkk0kmp6enT7a/kqQTGGVYTAFTVXVXm/8IvfB4pB1aor0f6lv/vL72K4EDrb5yQP04VbWtqiaqamL58uWzNhBJWuxGFhZV9X+Bh5O8uJUuAe4DdgKbWm0TcFub3glsTHJKktX0TmTvboeqHk+yvl0FdWVfG0nSGCwd8fZ/FfiDJM8Bvg68hV5A7UhyFbAfuAKgqvYm2UEvUI4A11TVU207VwM3A8uA29tLkjQmIw2LqroHmBiw6JITrL8V2DqgPglcMLu9kyQNy19wS5I6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6jRUWCTxJn6StIgNu2fxviS7k/xyktNH2iNJ0rwzVFhU1U8AP0/vSXaTSf4wyetG2jNJ0rwx9DmLqnoQ+E/AO4F/BtyQ5GtJ/uWoOidJmh+GPWfxsiS/CdwPvBb4F1X10jb9mzO0+0aSPUnuSTLZamcmuSPJg+39jL71r02yL8kDSS7tq69r29mX5Ib2eFVJ0pgMu2fx28CXgAur6pqq+hJAVR2gt7cxk9dU1cur6ugT87YAu6pqDbCrzZNkLbAROB+4DLgxyZLW5iZgM73ncq9pyyVJYzJsWLwB+MOq+juAJM9KcipAVX3wJD9zA7C9TW8HLu+r31pVh6vqIWAfcHGSc4HTqurOqirglr42kqQxGDYsPg0s65s/tdW6FPBnSe5OsrnVzqmqgwDt/exWXwE83Nd2qtVWtOlj68dJsjnJZJLJ6enpIbonSRrG0iHXe25VfefoTFV95+ieRYdXVdWBJGcDdyT52gzrDjoPUTPUjy9WbQO2AUxMTAxcR5J08obds3giyUVHZ5KsA/6uq1E7p0FVHQI+DlwMPNIOLdHeD7XVp+hdmnvUSuBAq68cUJckjcmwYfEO4MNJPp/k88AfAb8yU4Mkz0vygqPTwD8H7gV2ApvaapuA29r0TmBjklOSrKZ3Int3O1T1eJL17SqoK/vaSJLGYKjDUFX1xSQvAV5M77DQ16rq7zuanQN8vF3lupTeCfJPJfkisCPJVcB+4Ir2GXuT7ADuA44A11TVU21bVwM30ztvcnt7SZLGZNhzFgCvBFa1Nq9IQlXdcqKVq+rrwIUD6o8Cl5ygzVZg64D6JOD9qSRpjgwVFkk+CPwYcA9w9K/9o5exSpKe4Ybds5gA1rbfOUiSFplhT3DfC/zDUXZEkjR/DbtncRZwX5LdwOGjxap600h6JUmaV4YNi3ePshOSpPlt2EtnP5vkR4E1VfXp9uvtJV3tJEnPDMPeovytwEeA32mlFcAnRtUpSdL8MuwJ7muAVwGPwf9/ENLZM7aQJD1jDBsWh6vqyaMzSZZygpv5SZKeeYYNi88meRewrD17+8PAH4+uW5Kk+WTYsNgCTAN7gF8CPkn3E/IkSc8Qw14N9T3gf7SXJGmRGfbeUA8x4BxFVb1o1nskSZp3TubeUEc9l95txc+c/e5Ikuajoc5ZVNWjfa9vVdV7gdeOuG+SpHli2MNQF/XNPovensYLRtIjSdK8M+xhqP/WN30E+Abwr4dpmGQJMAl8q6p+JsmZ9B7Luurodqrqr9u61wJX0Xtmxq9V1Z+2+jq+/6S8TwJv93bpkjQ+wx6Gek3f63VV9daqemDIz3g7cH/f/BZgV1WtAXa1eZKsBTYC5wOXATe2oAG4CdhM77nca9pySdKYDHsY6t/PtLyqfuME7VYCb6T3qNSj29gAvLpNbwc+A7yz1W+tqsPAQ0n2ARcn+QZwWlXd2bZ5C3A5PodbksZm2B/lTQBX07uB4ArgbcBaeuctZjp38V7gPwLf66udU1UHAdr70XtMrQAe7ltvqu/zpgbUj5Nkc5LJJJPT09PDjUyS1OlkHn50UVU9DpDk3cCHq+rfnqhBkp8BDlXV3UlePcRnZECtZqgfX6zaBmwDmJiY8JyGJM2SYcPihcCTffNP0jtBPZNXAW9K8gZ6v804LcnvA48kObeqDiY5FzjU1p8CzutrvxI40OorB9QlSWMy7GGoDwK7k7w7yXXAXcAtMzWoqmuramVVraJ34vrPq+oXgJ3AprbaJuC2Nr0T2JjklCSr6Z3I3t0OVT2eZH2SAFf2tZEkjcGw94bamuR24Cdb6S1V9eWn+ZnXAzuSXAXsp/drcKpqb5IdwH30Ls+9pqqeam2u5vuXzt6OJ7claayGPQwFcCrwWFX9XpLlSVZX1UPDNKyqz9C76omqehS45ATrbaV35dSx9UnggpPoqyRpFg37WNXr6F3eem0rPRv4/VF1SpI0vwx7zuJngTcBTwBU1QG83YckLRrDhsWT7fYaBZDkeaPrkiRpvhk2LHYk+R3g9CRvBT6ND0KSpEWj8wR3u1z1j4CXAI8BLwZ+varuGHHfJEnzRGdYVFUl+URVrQMMCElahIY9DPWFJK8caU8kSfPWsL+zeA3wtnYH2Cfo3a+pquplo+qYJGn+mDEskrywqvYDrx9TfyRJ81DXnsUn6N1t9ptJPlpV/2ocnZIkzS9d5yz6bw/+olF2RJI0f3WFRZ1gWpK0iHQdhrowyWP09jCWtWn4/gnu00baO0nSvDBjWFTVknF1RJI0fw37OwtJ0iJmWEiSOo0sLJI8N8nuJF9JsjfJf271M5PckeTB9n5GX5trk+xL8kCSS/vq65LsactuaPerkiSNySj3LA4Dr62qC4GXA5clWQ9sAXZV1RpgV5snyVp6z+o+H7gMuDHJ0XMmNwGb6T2Xe01bLkkak5GFRfV8p80+u70K2ABsb/XtwOVtegNwa1Udbo9r3QdcnORc4LSqurM9U+OWvjaSpDEY6TmLJEuS3AMcAu6oqruAc6rqIEB7P7utvgJ4uK/5VKutaNPH1gd93uYkk0kmp6enZ3cwkrSIjTQsquqpqno5sJLeXsIFM6w+6DxEzVAf9HnbqmqiqiaWL19+8h2WJA00lquhqupvgM/QO9fwSDu0RHs/1FabAs7ra7YSONDqKwfUJUljMsqroZYnOb1NLwN+GvgasBPY1FbbBNzWpncCG5OckmQ1vRPZu9uhqseTrG9XQV3Z10aSNAbDPs/i6TgX2N6uaHoWsKOq/meSO+k90/sqYD9wBUBV7U2yA7gPOAJcU1VPtW1dDdwMLANuby9J0piMLCyq6qvAKwbUHwUuOUGbrcDWAfVJYKbzHZKkEfIX3JKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6jfKxqucl+V9J7k+yN8nbW/3MJHckebC9n9HX5tok+5I8kOTSvvq6JHvashva41UlSWMyyj2LI8B/qKqXAuuBa5KsBbYAu6pqDbCrzdOWbQTOBy4DbmyPZAW4CdhM77nca9pySdKYjCwsqupgVX2pTT8O3A+sADYA29tq24HL2/QG4NaqOlxVDwH7gIuTnAucVlV3VlUBt/S1kSSNwVjOWSRZRe953HcB51TVQegFCnB2W20F8HBfs6lWW9Gmj60P+pzNSSaTTE5PT8/mECRpURt5WCR5PvBR4B1V9dhMqw6o1Qz144tV26pqoqomli9ffvKdlSQNNNKwSPJsekHxB1X1sVZ+pB1aor0favUp4Ly+5iuBA62+ckBdkjQmo7waKsDvAvdX1W/0LdoJbGrTm4Db+uobk5ySZDW9E9m726Gqx5Osb9u8sq+NJGkMlo5w268CfhHYk+SeVnsXcD2wI8lVwH7gCoCq2ptkB3AfvSuprqmqp1q7q4GbgWXA7e0lSRqTkYVFVf1vBp9vALjkBG22AlsH1CeBC2avd5Kkk+EvuCVJnQwLSVKnUZ6zWLBWbfmTOfncb1z/xjn5XEnq4p6FJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKnTKB+r+oEkh5Lc21c7M8kdSR5s72f0Lbs2yb4kDyS5tK++LsmetuyG9mhVSdIYjXLP4mbgsmNqW4BdVbUG2NXmSbIW2Aic39rcmGRJa3MTsJneM7nXDNimJGnERhYWVfU54K+OKW8Atrfp7cDlffVbq+pwVT0E7AMuTnIucFpV3VlVBdzS10aSNCbjPmdxTlUdBGjvZ7f6CuDhvvWmWm1Fmz62PlCSzUkmk0xOT0/PasclaTGbLye4B52HqBnqA1XVtqqaqKqJ5cuXz1rnJGmxG3dYPNIOLdHeD7X6FHBe33orgQOtvnJAXZI0RuMOi53Apja9Cbitr74xySlJVtM7kb27Hap6PMn6dhXUlX1tJEljsnRUG07yIeDVwFlJpoDrgOuBHUmuAvYDVwBU1d4kO4D7gCPANVX1VNvU1fSurFoG3N5ekqQxGllYVNWbT7DokhOsvxXYOqA+CVwwi12TJJ2k+XKCW5I0jxkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqtGDCIsllSR5Isi/JlrnujyQtJgsiLJIsAf478HpgLfDmJGvntleStHgsiLAALgb2VdXXq+pJ4FZgwxz3SZIWjZE9g3uWrQAe7pufAv7JsSsl2QxsbrPfSfLA0/y8s4BvP822T1veM+5P/AFzMuY55pif+RbbeMl7fugx/+ig4kIJiwyo1XGFqm3Ath/6w5LJqpr4YbezkDjmxWGxjXmxjRdGN+aFchhqCjivb34lcGCO+iJJi85CCYsvAmuSrE7yHGAjsHOO+yRJi8aCOAxVVUeS/Arwp8AS4ANVtXeEH/lDH8pagBzz4rDYxrzYxgsjGnOqjjv0L0nSD1goh6EkSXPIsJAkdVrUYdF1C5H03NCWfzXJRXPRz9kyxHh/vo3zq0n+IsmFc9HP2TTsbWKSvDLJU0l+bpz9G4Vhxpzk1UnuSbI3yWfH3cfZNsT/7X+Q5I+TfKWN+S1z0c/ZkuQDSQ4lufcEy2f/u6uqFuWL3onyvwReBDwH+Aqw9ph13gDcTu93HuuBu+a63yMe748DZ7Tp1y/k8Q475r71/hz4JPBzc93vMfw7nw7cB7ywzZ891/0ew5jfBbynTS8H/gp4zlz3/YcY808BFwH3nmD5rH93LeY9i2FuIbIBuKV6vgCcnuTccXd0lnSOt6r+oqr+us1+gd7vWRayYW8T86vAR4FD4+zciAwz5n8DfKyq9gNU1UIf9zBjLuAFSQI8n15YHBlvN2dPVX2O3hhOZNa/uxZzWAy6hciKp7HOQnGyY7mK3l8mC1nnmJOsAH4WeN8Y+zVKw/w7/2PgjCSfSXJ3kivH1rvRGGbMvw28lN6PefcAb6+q742ne3Ni1r+7FsTvLEZkmFuIDHWbkQVi6LEkeQ29sPiJkfZo9IYZ83uBd1bVU70/Ohe8Yca8FFgHXAIsA+5M8oWq+j+j7tyIDDPmS4F7gNcCPwbckeTzVfXYqDs3R2b9u2sxh8UwtxB5Jt1mZKixJHkZ8H7g9VX16Jj6NirDjHkCuLUFxVnAG5IcqapPjKeLs27Y/9ffrqongCeSfA64EFioYTHMmN8CXF+9A/r7kjwEvATYPZ4ujt2sf3ct5sNQw9xCZCdwZbuyYD3wt1V1cNwdnSWd403yQuBjwC8u4L8y+3WOuapWV9WqqloFfAT45QUcFDDc/+vbgJ9MsjTJqfTu4Hz/mPs5m4YZ8356e1IkOQd4MfD1sfZyvGb9u2vR7lnUCW4hkuRtbfn76F0d8wZgH/Bden+dLEhDjvfXgR8Bbmx/aR+pBXzHziHH/IwyzJir6v4knwK+CnwPeH9VDbwEcyEY8t/5vwA3J9lD7xDNO6tqwd66PMmHgFcDZyWZAq4Dng2j++7ydh+SpE6L+TCUJGlIhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6vT/ABcob6vp1cTkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['attitude'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1e2c73960a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Correlation between review length and rating may be insightful\n",
    "df_ori['length of text'] = df_ori['text'].apply(len)\n",
    "graph = sns.FacetGrid(data=df_ori,col='stars')\n",
    "graph.map(plt.hist,'length of text',bins=50,color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can work more on this later to create more visuals and figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for data cleaning using regular expressions\n",
    "import re\n",
    "def function_clean(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #hyperlinks\n",
    "    text = re.sub(\"@[^\\s]*\", \"\", text)\n",
    "    text = re.sub(\"#[^\\s]*\", \"\", text)\n",
    "    text = re.sub('[0-9]*[+-:]*[0-9]+', '', text) #Dates\n",
    "    text = re.sub(\"'s\", \"\", text) # \" 's \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-2c6fb3533ba0>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda text: function_clean(text))\n"
     ]
    }
   ],
   "source": [
    "#Apply the regex function to text column\n",
    "df['text'] = df['text'].apply(lambda text: function_clean(text))\n",
    "X = df['text']\n",
    "y = df[\"attitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    listofwords = sentence.strip().split()          \n",
    "    listof_words = []    \n",
    "    for word in listofwords:\n",
    "        if not word in stop_words:\n",
    "            stem_word = porter.stem(word)\n",
    "            for punctuation_mark in string.punctuation:\n",
    "                stem_word = stem_word.replace(punctuation_mark, '').lower()\n",
    "            if len(word)>2:\n",
    "                listof_words.append(stem_word)\n",
    "    return(listof_words)\n",
    "\n",
    "#preprocess the text.\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
    "        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit()\n",
    "               and token not in punctuation and len(token) > 2]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this =  np.zeros(DIMENSION)\n",
    "        count_for_this = 0\n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "        if count_for_this == 0:\n",
    "            count_for_this = 1\n",
    "        feats.append(feat_for_this/count_for_this)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training/testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.5 s\n",
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "#Load the pre-trained word2vec model and the dataset\n",
    "data_path= r\"C:\\Users\\Steven Johannemann\\Desktop\\Capstone\\Capstone Notebooks\"\n",
    "path_to_model = os.path.join(data_path,'GoogleNews-vectors-negative300.bin')\n",
    "training_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n",
    "\n",
    "#Load W2V model. This will take some time. \n",
    "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "print('done loading Word2Vec')\n",
    "\n",
    "#Read text data, cats.\n",
    "#the file path consists of tab separated sentences and cats.\n",
    "train_texts = X_train.tolist()\n",
    "train_cats = y_train.tolist()\n",
    "test_texts = X_test.tolist()\n",
    "test_cats = y_test.tolist()\n",
    "#fh = open(training_data_path)\n",
    "#for line in fh:\n",
    "#    text, sentiment = line.split(\"\\t\")\n",
    "#    texts.append(text)\n",
    "#    cats.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model\n",
    "word2vec_vocab = w2v_model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one thing to note is that word embedding models do not require the same amount of cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using BoW, Tf-idf and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#Use CountVectorizer for BoW and TfidfVectorizer for Tf-idf\n",
    "cv = CountVectorizer(tokenizer = my_tokenizer, ngram_range=(1,3), min_df = 0.001).fit(X_train)\n",
    "tfidf = TfidfVectorizer(tokenizer = my_tokenizer, ngram_range=(1,3), min_df = 0.001).fit(X_train)\n",
    "X_traintfidf = tfidf.transform(X_train)\n",
    "X_testtfidf = tfidf.transform(X_test)\n",
    "X_traincv = cv.transform(X_train)\n",
    "X_testcv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_processed = preprocess_corpus(train_texts)\n",
    "train_texts_vectors = embedding_feats(train_texts_processed)\n",
    "\n",
    "test_texts_processed = preprocess_corpus(test_texts)\n",
    "test_texts_vectors = embedding_feats(test_texts_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview First 100 Feature Names\n",
    "#cv.get_feature_names()[0:100]\n",
    "#tfidf.get_feature_names()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 10359)\n",
      "(7000, 10359)\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "#Shape of the training sets after feature extraction\n",
    "print(X_traincv.shape)\n",
    "print(X_traintfidf.shape)\n",
    "print(len(train_texts_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5809\n",
      "1    5809\n",
      "Name: attitude, dtype: int64\n",
      "0    5809\n",
      "1    5809\n",
      "Name: attitude, dtype: int64\n",
      "0    5809\n",
      "1    5809\n",
      "Name: attitude, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=6)\n",
    "X_balcv, y_balcv = sm.fit_resample(X_traincv, y_train)\n",
    "X_baltfidf, y_baltfidf = sm.fit_resample(X_traintfidf, y_train)\n",
    "\n",
    "df_train_vect = DataFrame(train_texts_vectors,columns=[f\"{x}\" for x in range(0,300)])\n",
    "X_bal_w2v_train, y_bal_w2v_train = sm.fit_resample(df_train_vect, y_train)\n",
    "X_bal_w2v_test = DataFrame(test_texts_vectors,columns=[f\"{x}\" for x in range(0,300)])\n",
    "\n",
    "print(y_balcv.value_counts())\n",
    "print(y_baltfidf.value_counts())\n",
    "print(y_bal_w2v_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bal_w2v_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023744</td>\n",
       "      <td>0.060219</td>\n",
       "      <td>0.016482</td>\n",
       "      <td>0.090105</td>\n",
       "      <td>-0.007244</td>\n",
       "      <td>-0.025589</td>\n",
       "      <td>0.034035</td>\n",
       "      <td>-0.113440</td>\n",
       "      <td>0.054746</td>\n",
       "      <td>0.114717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056137</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.082917</td>\n",
       "      <td>0.063784</td>\n",
       "      <td>0.043157</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>0.054904</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015911</td>\n",
       "      <td>0.043575</td>\n",
       "      <td>-0.013652</td>\n",
       "      <td>0.100484</td>\n",
       "      <td>-0.005798</td>\n",
       "      <td>-0.015063</td>\n",
       "      <td>0.061395</td>\n",
       "      <td>-0.072637</td>\n",
       "      <td>0.067469</td>\n",
       "      <td>0.103962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038242</td>\n",
       "      <td>0.030037</td>\n",
       "      <td>-0.157944</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>-0.003957</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.006192</td>\n",
       "      <td>-0.096130</td>\n",
       "      <td>0.038877</td>\n",
       "      <td>-0.055206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046297</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>-0.013677</td>\n",
       "      <td>0.139245</td>\n",
       "      <td>-0.012362</td>\n",
       "      <td>0.013418</td>\n",
       "      <td>0.070208</td>\n",
       "      <td>-0.079559</td>\n",
       "      <td>0.020009</td>\n",
       "      <td>0.104837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003211</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>-0.082918</td>\n",
       "      <td>0.050714</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>-0.075983</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>-0.033747</td>\n",
       "      <td>0.014359</td>\n",
       "      <td>-0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.075425</td>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.172981</td>\n",
       "      <td>-0.074955</td>\n",
       "      <td>-0.002558</td>\n",
       "      <td>0.046766</td>\n",
       "      <td>-0.103122</td>\n",
       "      <td>-0.002265</td>\n",
       "      <td>0.140211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073666</td>\n",
       "      <td>0.068075</td>\n",
       "      <td>-0.044182</td>\n",
       "      <td>0.050248</td>\n",
       "      <td>0.045441</td>\n",
       "      <td>-0.017130</td>\n",
       "      <td>-0.008546</td>\n",
       "      <td>-0.096064</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.051083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.017591</td>\n",
       "      <td>0.019987</td>\n",
       "      <td>-0.016805</td>\n",
       "      <td>0.129858</td>\n",
       "      <td>-0.052706</td>\n",
       "      <td>-0.029344</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>-0.084492</td>\n",
       "      <td>0.057343</td>\n",
       "      <td>0.090267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029835</td>\n",
       "      <td>-0.000642</td>\n",
       "      <td>-0.075613</td>\n",
       "      <td>0.019254</td>\n",
       "      <td>-0.020711</td>\n",
       "      <td>-0.010652</td>\n",
       "      <td>-0.004084</td>\n",
       "      <td>-0.041545</td>\n",
       "      <td>0.014758</td>\n",
       "      <td>-0.033203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.023744  0.060219  0.016482  0.090105 -0.007244 -0.025589  0.034035   \n",
       "1 -0.015911  0.043575 -0.013652  0.100484 -0.005798 -0.015063  0.061395   \n",
       "2 -0.046297  0.020967 -0.013677  0.139245 -0.012362  0.013418  0.070208   \n",
       "3  0.008420  0.075425  0.024882  0.172981 -0.074955 -0.002558  0.046766   \n",
       "4 -0.017591  0.019987 -0.016805  0.129858 -0.052706 -0.029344  0.066977   \n",
       "\n",
       "          7         8         9  ...       290       291       292       293  \\\n",
       "0 -0.113440  0.054746  0.114717  ... -0.056137  0.021115 -0.082917  0.063784   \n",
       "1 -0.072637  0.067469  0.103962  ... -0.038242  0.030037 -0.157944  0.021459   \n",
       "2 -0.079559  0.020009  0.104837  ... -0.003211  0.003720 -0.082918  0.050714   \n",
       "3 -0.103122 -0.002265  0.140211  ... -0.073666  0.068075 -0.044182  0.050248   \n",
       "4 -0.084492  0.057343  0.090267  ... -0.029835 -0.000642 -0.075613  0.019254   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.043157 -0.026655 -0.040477 -0.025094  0.054904  0.021000  \n",
       "1 -0.003957 -0.000028 -0.006192 -0.096130  0.038877 -0.055206  \n",
       "2  0.012108 -0.075983  0.002162 -0.033747  0.014359 -0.000162  \n",
       "3  0.045441 -0.017130 -0.008546 -0.096064  0.009728  0.051083  \n",
       "4 -0.020711 -0.010652 -0.004084 -0.041545  0.014758 -0.033203  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bal_w2v_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap for Confidence intervals for B = 1000, n = len(predrmfr)\n",
    "#default 95% confidence, plot accuracy distribution\n",
    "#test and osr_pred are 1d arrays\n",
    "def confidence_bootstrap(test, ospred, conf = 0.95, b = 1000, acc = True):\n",
    "    sim_acc = []\n",
    "    sim_diff = []\n",
    "    n = len(ospred)\n",
    "    ossample = accuracy_score(test,ospred)\n",
    "    \n",
    "    df_data = {'real':test,'pred':ospred}\n",
    "    sample_data = pd.DataFrame(df_data, columns =['real','pred'])\n",
    "    \n",
    "    #simulation\n",
    "    for i in range(n):\n",
    "        bsample = sample_data.sample(n=len(ospred),replace=True)\n",
    "        sim_acc.append(accuracy_score(bsample['real'],bsample['pred']))\n",
    "        sim_diff.append(accuracy_score(bsample['real'],bsample['pred']) - ossample)\n",
    "    \n",
    "    sim_acc.sort()\n",
    "    sim_diff.sort()\n",
    "   \n",
    "    #onfidence Interval\n",
    "    lower = int(np.floor(n*(1-conf)/2))\n",
    "    upper = int(np.floor(n*(1+conf)/2))\n",
    "    \n",
    "    #Plots\n",
    "    if acc == False:\n",
    "        sns.distplot(sim_diff)\n",
    "        plt.xlabel(\"Difference\")\n",
    "        plt.title(\"Distribution of Difference of OSR2\")\n",
    "    else:\n",
    "        sns.distplot(sim_acc)\n",
    "        plt.xlabel(\"Accuracy\")\n",
    "        plt.title(\"Distribution of Accuracy\")\n",
    "    print(\"Confidence Interval:\", sim_acc[lower],sim_acc[upper])\n",
    "    print(\"Lower and Upper Quantiles:\",sim_diff[lower],sim_diff[upper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MN Naive Bayes Import\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      "[[ 368  117]\n",
      " [ 233 2282]]\n",
      "Score: 88.33\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68       485\n",
      "           1       0.95      0.91      0.93      2515\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.78      0.83      0.80      3000\n",
      "weighted avg       0.90      0.88      0.89      3000\n",
      "\n",
      "f1 score: 0.9287749287749286\n"
     ]
    }
   ],
   "source": [
    "#MN Naive Bayes using Bag of Words\n",
    "mnb.fit(X_balcv,y_balcv)\n",
    "predmnb = mnb.predict(X_testcv)\n",
    "print(\"Confusion Matrix for Multinomial Naive Bayes:\")\n",
    "print(confusion_matrix(y_test,predmnb))\n",
    "print(\"Score:\",round(accuracy_score(y_test,predmnb)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,predmnb))\n",
    "print(f'f1 score: {f1_score(y_test,predmnb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval: 0.872 0.8946666666666667\n",
      "Lower and Upper Quantiles: -0.011333333333333306 0.011333333333333417\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confidence_bootstrap(y_test.values.tolist(),predmnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      "[[ 410   75]\n",
      " [ 348 2167]]\n",
      "Score: 85.9\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.85      0.66       485\n",
      "           1       0.97      0.86      0.91      2515\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.75      0.85      0.79      3000\n",
      "weighted avg       0.90      0.86      0.87      3000\n",
      "\n",
      "f1 score: 0.911078410763086\n"
     ]
    }
   ],
   "source": [
    "#MN Naive Bayes using Tf-idf\n",
    "mnb.fit(X_baltfidf,y_baltfidf)\n",
    "predmnb = mnb.predict(X_testtfidf)\n",
    "print(\"Confusion Matrix for Multinomial Naive Bayes:\")\n",
    "print(confusion_matrix(y_test,predmnb))\n",
    "print(\"Score:\",round(accuracy_score(y_test,predmnb)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,predmnb))\n",
    "print(f'f1 score: {f1_score(y_test,predmnb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      "[[ 338  147]\n",
      " [ 654 1861]]\n",
      "Score: 73.3\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.70      0.46       485\n",
      "           1       0.93      0.74      0.82      2515\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.63      0.72      0.64      3000\n",
      "weighted avg       0.83      0.73      0.76      3000\n",
      "\n",
      "f1 score: 0.8229051514481539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(X_bal_w2v_train)\n",
    "scaled_test = scaler.fit_transform(X_bal_w2v_test)\n",
    "#MN Naive Bayes using Word2vec\n",
    "mnb.fit(scaled_train, y_bal_w2v_train)\n",
    "predmnb = mnb.predict(scaled_test)\n",
    "print(\"Confusion Matrix for Multinomial Naive Bayes:\")\n",
    "print(confusion_matrix(y_test,predmnb))\n",
    "print(\"Score:\",round(accuracy_score(y_test,predmnb)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,predmnb))\n",
    "print(f'f1 score: {f1_score(y_test,predmnb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Random Forest\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nreview_random_forest = RandomForestClassifier(random_state=6)\\nreview_random_forest.fit(X_bal_w2v_train, y_bal_w2v_train)\\nherro = review_random_forest.predict(X_bal_w2v_test)\\nprint(f\"Random Forest: {review_random_forest.score(X_bal_w2v_train, y_bal_w2v_train)}\")\\n# Random Forest: 0.9998278533310381\\nprint(f\"Random Forest: {review_random_forest.score(X_bal_w2v_test, y_test)}\")\\n# Random Forest: 0.874\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "review_random_forest = RandomForestClassifier(random_state=6)\n",
    "review_random_forest.fit(X_bal_w2v_train, y_bal_w2v_train)\n",
    "herro = review_random_forest.predict(X_bal_w2v_test)\n",
    "print(f\"Random Forest: {review_random_forest.score(X_bal_w2v_train, y_bal_w2v_train)}\")\n",
    "# Random Forest: 0.9998278533310381\n",
    "print(f\"Random Forest: {review_random_forest.score(X_bal_w2v_test, y_test)}\")\n",
    "# Random Forest: 0.874\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Confusion Matrix for Multinomial Naive Bayes:\")\\nprint(confusion_matrix(y_test,herro))\\nprint(\"Score:\",round(accuracy_score(y_test,herro)*100,2))\\nprint(\"Classification Report:\",classification_report(y_test,herro))\\nprint(f\\'f1 score: {f1_score(y_test,herro)}\\')\\n# Tuning Hyperparameters: n_estimators, max_features, min_sample_leaf\\n# https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"Confusion Matrix for Multinomial Naive Bayes:\")\n",
    "print(confusion_matrix(y_test,herro))\n",
    "print(\"Score:\",round(accuracy_score(y_test,herro)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,herro))\n",
    "print(f'f1 score: {f1_score(y_test,herro)}')\n",
    "# Tuning Hyperparameters: n_estimators, max_features, min_sample_leaf\n",
    "# https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty = 'l1', solver='saga',random_state=6, C =10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression:\n",
      "[[ 318  167]\n",
      " [ 284 2231]]\n",
      "Score: 84.97\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.66      0.59       485\n",
      "           1       0.93      0.89      0.91      2515\n",
      "\n",
      "    accuracy                           0.85      3000\n",
      "   macro avg       0.73      0.77      0.75      3000\n",
      "weighted avg       0.87      0.85      0.86      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "#Lasso Regression using Bag of Words\n",
    "logreg.fit(X_balcv,y_balcv)\n",
    "predlog = logreg.predict(X_testcv)\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(confusion_matrix(y_test,predlog))\n",
    "print(\"Score:\",round(accuracy_score(y_test,predlog)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,predlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression:\n",
      "[[ 343  142]\n",
      " [ 248 2267]]\n",
      "Score: 87.0\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.71      0.64       485\n",
      "           1       0.94      0.90      0.92      2515\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.76      0.80      0.78      3000\n",
      "weighted avg       0.88      0.87      0.88      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "#Lasso Regression using Tf-idf\n",
    "logreg.fit(X_baltfidf,y_baltfidf)\n",
    "predlog = logreg.predict(X_testtfidf)\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(confusion_matrix(y_test,predlog))\n",
    "print(\"Score:\",round(accuracy_score(y_test,predlog)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,predlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression:\n",
      "[[ 385  100]\n",
      " [ 368 2147]]\n",
      "Score: 84.4\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.79      0.62       485\n",
      "           1       0.96      0.85      0.90      2515\n",
      "\n",
      "    accuracy                           0.84      3000\n",
      "   macro avg       0.73      0.82      0.76      3000\n",
      "weighted avg       0.88      0.84      0.86      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "#Lasso Regression using Word2vec ##########################WHY DOES SCALED WORK BETTER?\n",
    "logreg.fit(X_bal_w2v_train, y_bal_w2v_train)\n",
    "predlog = logreg.predict(X_bal_w2v_test)\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(confusion_matrix(y_test,predlog))\n",
    "print(\"Score:\",round(accuracy_score(y_test,predlog)*100,2))\n",
    "print(\"Classification Report:\",classification_report(y_test,predlog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Import\n",
    "import xgboost as xgb#!pip install hyperopt\n",
    "import hyperopt as hyp\n",
    "from hyperopt import fmin, tpe, hp, anneal, Trials\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "random_state = 123\n",
    "n_iter=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_mse_cv(params, random_state=123, cv=5, X=X_balcv, y=y_balcv):\n",
    "    # the function gets a set of variable parameters in \"param\"\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'learning_rate': params['learning_rate']}\n",
    "    # we use this params to create a new LGBM Regressor\n",
    "    model = xgb.XGBClassifier(random_state=random_state, **params)\n",
    "    # and then conduct the cross validation with the same folds as before\n",
    "    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'n_estimators': hp.quniform('n_estimators', 30, 2000, 1),\n",
    "       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n",
    "       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:21<00:00, 58.07s/trial, best loss: 0.11187855783258485]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best=fmin(fn=gb_mse_cv, # function to optimize\n",
    "          space=space, \n",
    "          algo=hyp.rand.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n",
    "          max_evals=n_iter, # maximum number of iterations\n",
    "          trials=trials, # logging\n",
    "          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best MSE 0.112 params {'learning_rate': 0.008540588799230704, 'max_depth': 5.0, 'n_estimators': 1543.0}\n"
     ]
    }
   ],
   "source": [
    "# computing the score on the test set\n",
    "model = xgb.XGBClassifier(random_state=random_state, n_estimators=int(best['n_estimators']),\n",
    "                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\n",
    "model.fit(X_balcv,y_balcv)\n",
    "rand_test_score=mean_squared_error(y_test, model.predict(X_testcv))\n",
    "xgb_BoW_predictions = model.predict(X_testcv)\n",
    "\n",
    "print(\"Best MSE {:.3f} params {}\".format( gb_mse_cv(best), best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      "[[ 150  335]\n",
      " [  34 2481]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.31      0.45       485\n",
      "           1       0.88      0.99      0.93      2515\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.85      0.65      0.69      3000\n",
      "weighted avg       0.87      0.88      0.85      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9307822172200337"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion Matrix for Multinomial Naive Bayes:\")\n",
    "print(confusion_matrix(y_test,xgb_BoW_predictions))\n",
    "print(classification_report(y_test, xgb_BoW_predictions))\n",
    "f1_score(y_test, xgb_BoW_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:40:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.46      0.58       485\n",
      "           1       0.90      0.98      0.94      2515\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.85      0.72      0.76      3000\n",
      "weighted avg       0.89      0.89      0.88      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9386819484240688"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost using Bag of Words\n",
    "model = xgb.XGBClassifier(random_state = 123)\n",
    "model.fit(X_balcv,y_balcv)\n",
    "from sklearn.metrics import f1_score\n",
    "xgb_predictions = model.predict(X_testcv)\n",
    "print(classification_report(y_test, xgb_predictions))\n",
    "f1_score(y_test, xgb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_mse_cv1(params, random_state=123, cv=5, X=X_baltfidf, y=y_baltfidf):\n",
    "    # the function gets a set of variable parameters in \"param\"\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'learning_rate': params['learning_rate']}\n",
    "    # we use this params to create a new LGBM Regressor\n",
    "    model = xgb.XGBClassifier(random_state=random_state, **params)\n",
    "    # and then conduct the cross validation with the same folds as before\n",
    "    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'n_estimators': hp.quniform('n_estimators', 30, 2000, 1),\n",
    "       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n",
    "       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [49:53<00:00, 149.69s/trial, best loss: 0.08081569250990803] \n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best=fmin(fn=gb_mse_cv1, # function to optimize\n",
    "          space=space, \n",
    "          algo=hyp.rand.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n",
    "          max_evals=n_iter, # maximum number of iterations\n",
    "          trials=trials, # logging\n",
    "          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:30:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best MSE 0.116 params {'learning_rate': 0.022054333903620153, 'max_depth': 13.0, 'n_estimators': 1464.0}\n"
     ]
    }
   ],
   "source": [
    "# computing the score on the test set\n",
    "model = xgb.XGBClassifier(random_state=random_state, n_estimators=int(best['n_estimators']),\n",
    "                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\n",
    "model.fit(X_baltfidf,y_baltfidf)\n",
    "rand_test_score=mean_squared_error(y_test, model.predict(X_testtfidf))\n",
    "xgb_tfidf_predictions = model.predict(X_testtfidf)\n",
    "\n",
    "print(\"Best MSE {:.3f} params {}\".format( gb_mse_cv(best), best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.53      0.59       485\n",
      "           1       0.91      0.95      0.93      2515\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.79      0.74      0.76      3000\n",
      "weighted avg       0.87      0.88      0.88      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9304229195088677"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, xgb_tfidf_predictions))\n",
    "f1_score(y_test, xgb_tfidf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:33:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.54      0.58       485\n",
      "           1       0.91      0.94      0.93      2515\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.77      0.74      0.76      3000\n",
      "weighted avg       0.87      0.88      0.87      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9266954135633084"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost using Tf-idf\n",
    "model = xgb.XGBClassifier(random_state = 123)\n",
    "model.fit(X_baltfidf,y_baltfidf)\n",
    "from sklearn.metrics import f1_score\n",
    "xgb_predictions = model.predict(X_testtfidf)\n",
    "print(classification_report(y_test, xgb_predictions))\n",
    "f1_score(y_test, xgb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_mse_cv2(params, random_state=123, cv=5, X=X_bal_w2v_train, y=y_bal_w2v_train):\n",
    "    # the function gets a set of variable parameters in \"param\"\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'learning_rate': params['learning_rate']}\n",
    "    # we use this params to create a new LGBM Regressor\n",
    "    model = xgb.XGBClassifier(random_state=random_state, **params)\n",
    "    # and then conduct the cross validation with the same folds as before\n",
    "    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'n_estimators': hp.quniform('n_estimators', 30, 2000, 1),\n",
    "       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n",
    "       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:18:59<00:00, 236.97s/trial, best loss: 0.04785524238272813]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best=fmin(fn=gb_mse_cv2, # function to optimize\n",
    "          space=space, \n",
    "          algo=hyp.rand.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n",
    "          max_evals=n_iter, # maximum number of iterations\n",
    "          trials=trials, # logging\n",
    "          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:52:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best MSE 0.120 params {'learning_rate': 0.17771918355622215, 'max_depth': 13.0, 'n_estimators': 842.0}\n"
     ]
    }
   ],
   "source": [
    "# computing the score on the test set\n",
    "model = xgb.XGBClassifier(random_state=random_state, n_estimators=int(best['n_estimators']),\n",
    "                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\n",
    "\n",
    "\n",
    "model.fit(X_bal_w2v_train,y_bal_w2v_train)\n",
    "rand_test_score=mean_squared_error(y_test, model.predict(X_bal_w2v_test))\n",
    "xgb_w2v_predictions = model.predict(X_bal_w2v_test)\n",
    "\n",
    "print(\"Best MSE {:.3f} params {}\".format( gb_mse_cv(best), best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61       485\n",
      "           1       0.92      0.94      0.93      2515\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.78      0.76      0.77      3000\n",
      "weighted avg       0.88      0.88      0.88      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.929893658920835"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, xgb_w2v_predictions))\n",
    "f1_score(y_test, xgb_w2v_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:53:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven Johannemann\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59       485\n",
      "           1       0.92      0.92      0.92      2515\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.76      0.76      0.76      3000\n",
      "weighted avg       0.87      0.87      0.87      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9225883287018658"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost using Word2vec\n",
    "model = xgb.XGBClassifier(random_state = 123)\n",
    "model.fit(X_bal_w2v_train,y_bal_w2v_train)\n",
    "from sklearn.metrics import f1_score\n",
    "xgb_predictions = model.predict(X_bal_w2v_test)\n",
    "print(classification_report(y_test, xgb_predictions))\n",
    "f1_score(y_test, xgb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02374415  0.06021875  0.01648218 ... -0.02509382  0.05490401\n",
      "   0.02100026]\n",
      " [-0.0159111   0.04357529 -0.01365153 ... -0.09613037  0.03887717\n",
      "  -0.0552063 ]\n",
      " [-0.04629723  0.02096696 -0.01367742 ... -0.03374745  0.0143588\n",
      "  -0.00016174]\n",
      " ...\n",
      " [ 0.03773635  0.04661243 -0.00347702 ... -0.03197458  0.03983109\n",
      "  -0.01870567]\n",
      " [-0.00652099  0.02587429  0.0226821  ... -0.04240371 -0.01293024\n",
      "  -0.00561666]\n",
      " [ 0.00127035  0.04852371 -0.00530757 ... -0.01893766  0.05268669\n",
      "  -0.00287938]]\n"
     ]
    }
   ],
   "source": [
    "print(X_bal_w2v_train.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = DataFrame(xgb_BoW_predictions, columns =['predictions'])\n",
    "frames = [X_test.reset_index(), y_test.reset_index(), results]\n",
    "result = pd.concat(frames, axis = 1)\n",
    "results_0 = result.loc[result['predictions'] == 0]\n",
    "#results_0.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     This place used to be amazing.  My favorite me...\n",
       "31    Went into the location and waited minutes for ...\n",
       "68    What was Dunkin Donuts thinking when they took...\n",
       "76    We ordered $ worth of pizza for takeout and it...\n",
       "80                   Poor service-small portions-pricey\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_text = results_0['text']\n",
    "LDA_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_nan_removed = results_0[~results_0.text.isnull()]\n",
    "#print(result_nan_removed)\n",
    "#result_nan_removed['attitude'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model https://shichaoji.com/tag/topic-modeling-python-lda-visualization-gensim-pyldavis-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Steven\n",
      "[nltk_data]     Johannemann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Steven\n",
      "[nltk_data]     Johannemann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation) \n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "def cleaning(article):\n",
    "    one = \" \".join([i for i in article.lower().split() if i not in stopwords])\n",
    "    two = \"\".join(i for i in one if i not in punctuation)\n",
    "    three = \" \".join(lemmatize.lemmatize(i) for i in two.split())\n",
    "    return three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     This place used to be amazing.  My favorite me...\n",
       "31    Went into the location and waited minutes for ...\n",
       "68    What was Dunkin Donuts thinking when they took...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv('yelp_neg_rev.csv', error_bad_lines=False);\n",
    "#data_text = data[['text']]\n",
    "#text = data_text.applymap(cleaning)['text']\n",
    "text = LDA_text.to_frame().applymap(cleaning)['text']\n",
    "text_list = [i.split() for i in text]\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place',\n",
       " 'used',\n",
       " 'amazing',\n",
       " 'favorite',\n",
       " 'memory',\n",
       " 'past',\n",
       " 'year',\n",
       " 'going',\n",
       " 'seafood',\n",
       " 'place',\n",
       " 'parent',\n",
       " 'anymore',\n",
       " 'went',\n",
       " 'wife',\n",
       " 'th',\n",
       " 'birthday',\n",
       " 'dad',\n",
       " 'decided',\n",
       " 'join',\n",
       " 'u',\n",
       " 'wife',\n",
       " 'ordered',\n",
       " 'steak',\n",
       " 'shrimp',\n",
       " 'dad',\n",
       " 'ordered',\n",
       " 'steak',\n",
       " 'ordered',\n",
       " 'crab',\n",
       " 'stuffed',\n",
       " 'lobster',\n",
       " 'crab',\n",
       " 'stuffed',\n",
       " 'lobster',\n",
       " 'many',\n",
       " 'time',\n",
       " 'before',\n",
       " 'craving',\n",
       " 'it',\n",
       " 'got',\n",
       " 'appetizer',\n",
       " 'dad',\n",
       " 'got',\n",
       " 'snail',\n",
       " 'wife',\n",
       " 'shared',\n",
       " 'steamer',\n",
       " 'dad',\n",
       " 'happy',\n",
       " 'snail',\n",
       " 'again',\n",
       " 'go',\n",
       " 'wrong',\n",
       " 'snail',\n",
       " 'doused',\n",
       " 'garlic',\n",
       " 'butter',\n",
       " 'steamer',\n",
       " 'steamed',\n",
       " 'little',\n",
       " 'neck',\n",
       " 'clam',\n",
       " 'garlic',\n",
       " 'broth',\n",
       " 'long',\n",
       " 'neck',\n",
       " 'clam',\n",
       " 'like',\n",
       " 'rest',\n",
       " 'world',\n",
       " 'call',\n",
       " 'steamer',\n",
       " 'argue',\n",
       " 'server',\n",
       " 'wrong',\n",
       " 'clam',\n",
       " 'food',\n",
       " 'came',\n",
       " 'first',\n",
       " 'time',\n",
       " 'life',\n",
       " 'asking',\n",
       " 'server',\n",
       " 'take',\n",
       " 'back',\n",
       " 'meal',\n",
       " 'even',\n",
       " 'tried',\n",
       " 'it',\n",
       " 'looked',\n",
       " 'dry',\n",
       " 'cooked',\n",
       " 'told',\n",
       " 'server',\n",
       " 'ive',\n",
       " 'dish',\n",
       " 'many',\n",
       " 'time',\n",
       " 'way',\n",
       " 'supposed',\n",
       " 'be',\n",
       " 'told',\n",
       " 'supposed',\n",
       " 'look',\n",
       " 'like',\n",
       " 'begged',\n",
       " 'try',\n",
       " 'it',\n",
       " 'took',\n",
       " 'one',\n",
       " 'bite',\n",
       " 'even',\n",
       " 'swallow',\n",
       " 'dry',\n",
       " 'course',\n",
       " 'server',\n",
       " 'gone',\n",
       " 'point',\n",
       " 'come',\n",
       " 'back',\n",
       " 'minute',\n",
       " 'ask',\n",
       " 'wa',\n",
       " 'waiting',\n",
       " 'server',\n",
       " 'return',\n",
       " 'told',\n",
       " 'family',\n",
       " 'going',\n",
       " 'get',\n",
       " 'steak',\n",
       " 'shook',\n",
       " 'head',\n",
       " 'told',\n",
       " 'to',\n",
       " 'took',\n",
       " 'one',\n",
       " 'bite',\n",
       " 'dad',\n",
       " 'steak',\n",
       " 'spat',\n",
       " 'out',\n",
       " 'screw',\n",
       " 'steak',\n",
       " 'bad',\n",
       " 'tasted',\n",
       " 'like',\n",
       " 'beef',\n",
       " 'jerky',\n",
       " 'butter',\n",
       " 'dad',\n",
       " 'wife',\n",
       " 'would',\n",
       " 'complained',\n",
       " 'too',\n",
       " 'wife',\n",
       " 'birthday',\n",
       " 'already',\n",
       " 'stealing',\n",
       " 'thunder',\n",
       " 'swapped',\n",
       " 'meal',\n",
       " 'steamed',\n",
       " 'lobster',\n",
       " 'screw',\n",
       " 'steamed',\n",
       " 'lobster',\n",
       " 'fortunately',\n",
       " 'came',\n",
       " 'fine',\n",
       " 'left',\n",
       " 'hungry',\n",
       " 'poorer',\n",
       " 'used',\n",
       " 'mind',\n",
       " 'paying',\n",
       " 'money',\n",
       " 'charged',\n",
       " 'now',\n",
       " 'worth',\n",
       " 'it',\n",
       " 'really',\n",
       " 'hoping',\n",
       " 'people',\n",
       " 'stop',\n",
       " 'coming',\n",
       " 'close',\n",
       " 'down']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,\n",
    "                   filename='running.log',filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3951 unique tokens: ['again', 'already', 'amazing', 'anymore', 'appetizer']...)\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(text_list)\n",
    "dictionary.save('dictionary.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "[(8, 1), (23, 1), (46, 1), (68, 1), (73, 1), (75, 1), (80, 3), (83, 1), (100, 1), (118, 1), (122, 1), (149, 1), (160, 1), (184, 1), (198, 1), (202, 1), (225, 1), (398, 1), (463, 1), (503, 1), (517, 1), (528, 1), (550, 2), (553, 1), (604, 1), (632, 1), (734, 1), (827, 1), (915, 1), (1000, 1), (1206, 1), (1470, 1), (1517, 1), (1534, 1), (1536, 1), (1823, 1), (1905, 1), (1915, 1), (2345, 2), (2618, 1), (2724, 1), (2725, 1), (2726, 1), (2727, 2), (2728, 1), (2729, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
    "corpora.MmCorpus.serialize('corpus.mm', doc_term_matrix)\n",
    "\n",
    "print(len(doc_term_matrix))\n",
    "print (doc_term_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 6.18s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)\n",
    "print('used: {:.2f}s'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '0.008*\"service\" + 0.007*\"place\" + 0.006*\"bag\" + 0.006*\"donut\"'), (2, '0.010*\"place\" + 0.010*\"food\" + 0.007*\"one\" + 0.006*\"order\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.009*\"one\" + 0.007*\"like\" + 0.006*\"im\" + 0.006*\"would\" + 0.005*\"back\" + 0.005*\"time\" + 0.005*\"service\" + 0.005*\"me\" + 0.005*\"told\" + 0.004*\"u\"\n",
      "1\n",
      "0.008*\"service\" + 0.007*\"place\" + 0.006*\"bag\" + 0.006*\"donut\" + 0.006*\"time\" + 0.006*\"would\" + 0.004*\"like\" + 0.004*\"pasta\" + 0.004*\"say\" + 0.004*\"one\"\n",
      "2\n",
      "0.010*\"place\" + 0.010*\"food\" + 0.007*\"one\" + 0.006*\"order\" + 0.006*\"never\" + 0.006*\"me\" + 0.006*\"would\" + 0.005*\"like\" + 0.005*\"pizza\" + 0.005*\"guy\"\n",
      "3\n",
      "0.008*\"food\" + 0.007*\"time\" + 0.006*\"get\" + 0.006*\"wait\" + 0.005*\"item\" + 0.004*\"u\" + 0.004*\"go\" + 0.004*\"told\" + 0.004*\"menu\" + 0.004*\"price\"\n",
      "4\n",
      "0.011*\"food\" + 0.009*\"like\" + 0.008*\"u\" + 0.008*\"one\" + 0.008*\"get\" + 0.008*\"time\" + 0.007*\"place\" + 0.007*\"good\" + 0.007*\"table\" + 0.006*\"would\"\n"
     ]
    }
   ],
   "source": [
    "for i in ldamodel.print_topics(): \n",
    "    for j in i: print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "loading = LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.009*\"one\" + 0.007*\"like\" + 0.006*\"im\" + 0.006*\"would\"'), (3, '0.008*\"food\" + 0.007*\"time\" + 0.006*\"get\" + 0.006*\"wait\"')]\n"
     ]
    }
   ],
   "source": [
    "print(loading.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_new(doc):\n",
    "    one = cleaning(doc).split()\n",
    "    two = dictionary.doc2bow(one)\n",
    "    return two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(171, 1), (3188, 1), (3625, 1)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_new('new article that to be classified by trained model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.7988607),\n",
       " (1, 0.050193563),\n",
       " (2, 0.05023016),\n",
       " (3, 0.050352823),\n",
       " (4, 0.05036272)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belong = loading[(pre_new('new article that to be classified by trained model!'))]\n",
    "belong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.798861</td>\n",
       "      <td>0.009*\"one\" + 0.007*\"like\" + 0.006*\"im\" + 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>0.011*\"food\" + 0.009*\"like\" + 0.008*\"u\" + 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.050353</td>\n",
       "      <td>0.008*\"food\" + 0.007*\"time\" + 0.006*\"get\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.050230</td>\n",
       "      <td>0.010*\"place\" + 0.010*\"food\" + 0.007*\"one\" + 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.050194</td>\n",
       "      <td>0.008*\"service\" + 0.007*\"place\" + 0.006*\"bag\" ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      prob                                              topic\n",
       "0   0  0.798861  0.009*\"one\" + 0.007*\"like\" + 0.006*\"im\" + 0.00...\n",
       "4   4  0.050363  0.011*\"food\" + 0.009*\"like\" + 0.008*\"u\" + 0.00...\n",
       "3   3  0.050353  0.008*\"food\" + 0.007*\"time\" + 0.006*\"get\" + 0....\n",
       "2   2  0.050230  0.010*\"place\" + 0.010*\"food\" + 0.007*\"one\" + 0...\n",
       "1   1  0.050194  0.008*\"service\" + 0.007*\"place\" + 0.006*\"bag\" ..."
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.DataFrame(belong,columns=['id','prob']).sort_values('prob',ascending=False)\n",
    "new['topic'] = new['id'].apply(loading.print_topic)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.009*\"one\" + 0.007*\"like\" + 0.006*\"im\" + 0.00...\n",
       "4    0.011*\"food\" + 0.009*\"like\" + 0.008*\"u\" + 0.00...\n",
       "3    0.008*\"food\" + 0.007*\"time\" + 0.006*\"get\" + 0....\n",
       "2    0.010*\"place\" + 0.010*\"food\" + 0.007*\"one\" + 0...\n",
       "1    0.008*\"service\" + 0.007*\"place\" + 0.006*\"bag\" ...\n",
       "Name: topic, dtype: object"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gensim.corpora.Dictionary.load('dictionary.dict')\n",
    "c = gensim.corpora.MmCorpus('corpus.mm')\n",
    "lda = gensim.models.LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2960020730069778725960769360\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2960020730069778725960769360_data = {\"mdsDat\": {\"x\": [-0.003068580186258013, 0.07305674538313903, -0.10170727128324022, 0.049143506034287014, -0.01742439994792779], \"y\": [-0.06417880322333173, -0.06021766942452753, -0.014992518424941577, 0.07670799394313628, 0.06268099712966455], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [62.44655464549119, 13.69096550202816, 9.359234192783594, 7.315106663297747, 7.188138996399308]}, \"tinfo\": {\"Term\": [\"food\", \"place\", \"time\", \"service\", \"one\", \"would\", \"wait\", \"told\", \"me\", \"order\", \"restaurant\", \"never\", \"get\", \"good\", \"like\", \"pizza\", \"pasta\", \"im\", \"item\", \"back\", \"guy\", \"bag\", \"go\", \"price\", \"waiter\", \"say\", \"going\", \"right\", \"menu\", \"slice\", \"taco\", \"thai\", \"bread\", \"fry\", \"leave\", \"meal\", \"group\", \"weird\", \"son\", \"beef\", \"little\", \"spicy\", \"entree\", \"seated\", \"sweet\", \"mom\", \"mind\", \"seat\", \"down\", \"hungry\", \"plate\", \"roll\", \"wife\", \"child\", \"polish\", \"didnt\", \"completely\", \"tiny\", \"dad\", \"today\", \"table\", \"kid\", \"server\", \"tasted\", \"nice\", \"came\", \"really\", \"got\", \"good\", \"people\", \"since\", \"bar\", \"food\", \"drink\", \"u\", \"get\", \"well\", \"ordered\", \"even\", \"like\", \"time\", \"it\", \"ever\", \"worst\", \"asked\", \"one\", \"restaurant\", \"said\", \"place\", \"thing\", \"back\", \"go\", \"would\", \"went\", \"service\", \"minute\", \"order\", \"going\", \"wm\", \"body\", \"nobuo\", \"butai\", \"host\", \"gallo\", \"blanco\", \"charcoal\", \"towel\", \"thats\", \"data\", \"crystal\", \"presentation\", \"bride\", \"mousse\", \"robson\", \"oj\", \"slap\", \"h\", \"uncommon\", \"melted\", \"prescription\", \"circle\", \"costco\", \"hardwood\", \"drastically\", \"furthermore\", \"bubble\", \"male\", \"freezer\", \"rib\", \"cousin\", \"certain\", \"room\", \"mother\", \"chocolate\", \"employee\", \"regular\", \"certainly\", \"dessert\", \"kept\", \"slice\", \"pizza\", \"cream\", \"showed\", \"getting\", \"hour\", \"im\", \"one\", \"me\", \"store\", \"find\", \"ice\", \"customer\", \"star\", \"told\", \"rude\", \"like\", \"back\", \"would\", \"ive\", \"service\", \"time\", \"could\", \"said\", \"u\", \"good\", \"minute\", \"get\", \"place\", \"wanted\", \"another\", \"take\", \"it\", \"lee\", \"tempura\", \"session\", \"fail\", \"sucked\", \"assorted\", \"duck\", \"exceeded\", \"mimi\", \"chinese\", \"visiting\", \"sore\", \"training\", \"sell\", \"l\", \"richardson\", \"rokerij\", \"facial\", \"rockerij\", \"dick\", \"placing\", \"patronize\", \"can\", \"funny\", \"yakisoba\", \"seasonal\", \"r\", \"medium\", \"lard\", \"effort\", \"expectation\", \"music\", \"fitness\", \"slice\", \"topping\", \"stale\", \"pizza\", \"status\", \"dollar\", \"me\", \"commission\", \"place\", \"never\", \"buy\", \"guy\", \"outside\", \"order\", \"food\", \"waiter\", \"one\", \"would\", \"went\", \"no\", \"right\", \"shrimp\", \"back\", \"chip\", \"like\", \"wait\", \"good\", \"im\", \"going\", \"come\", \"service\", \"give\", \"minute\", \"better\", \"ordered\", \"u\", \"restaurant\", \"again\", \"donut\", \"root\", \"movie\", \"insurance\", \"matthew\", \"co\", \"surgery\", \"bagging\", \"checkout\", \"attendant\", \"personal\", \"serrano\", \"bosa\", \"estimate\", \"penne\", \"fri\", \"denver\", \"hopefully\", \"sec\", \"wear\", \"viewing\", \"limited\", \"syrup\", \"otherwise\", \"stevia\", \"rb\", \"settle\", \"denny\", \"dozen\", \"attentive\", \"bag\", \"total\", \"pasta\", \"credit\", \"wing\", \"small\", \"service\", \"phoenix\", \"employee\", \"feel\", \"fit\", \"dr\", \"received\", \"beer\", \"place\", \"say\", \"would\", \"sure\", \"time\", \"told\", \"like\", \"one\", \"better\", \"pizza\", \"experience\", \"go\", \"nothing\", \"u\", \"item\", \"could\", \"horrible\", \"food\", \"takeout\", \"plane\", \"sapporo\", \"sampler\", \"baklava\", \"missed\", \"lax\", \"smoothie\", \"jungle\", \"screwing\", \"inform\", \"mechanical\", \"agent\", \"obligatory\", \"luggage\", \"ticketing\", \"afford\", \"band\", \"monday\", \"fraction\", \"tokyo\", \"delta\", \"everybody\", \"weekly\", \"city\", \"pho\", \"listed\", \"maintenance\", \"broken\", \"record\", \"flight\", \"item\", \"wait\", \"internet\", \"business\", \"dish\", \"price\", \"promised\", \"pita\", \"time\", \"food\", \"early\", \"morning\", \"menu\", \"get\", \"line\", \"wrong\", \"offered\", \"told\", \"restaurant\", \"machine\", \"go\", \"hour\", \"minute\", \"going\", \"u\", \"order\", \"would\", \"said\", \"ordered\", \"chicken\", \"one\", \"place\", \"back\", \"got\"], \"Freq\": [161.0, 119.0, 117.0, 85.0, 130.0, 101.0, 32.0, 59.0, 46.0, 69.0, 48.0, 56.0, 109.0, 96.0, 137.0, 28.0, 15.0, 62.0, 21.0, 89.0, 37.0, 14.0, 69.0, 25.0, 22.0, 44.0, 56.0, 39.0, 39.0, 15.0, 21.58495450831272, 18.794803969662368, 17.862082459069953, 16.934707099647575, 15.068191184689026, 30.891249494056318, 13.21039318584549, 13.20924047924226, 12.277301396060437, 12.275312617357928, 27.152290580242923, 13.114277604604476, 11.351374443232585, 11.35137051801146, 11.342983628877851, 10.420813112665067, 9.488747768203778, 9.487878985928472, 9.486900297461709, 9.48609889814902, 18.79641200191591, 18.780974107237682, 8.559696339361718, 8.559695030954677, 8.559489611049221, 8.551817112160066, 18.734755936916994, 8.527130742311648, 7.629141550829407, 7.6291376256082835, 73.75197407642177, 14.143034883608395, 42.98933576087983, 20.464850791732687, 21.966814411671976, 47.6045165524358, 48.88760330050056, 56.899900731863454, 79.10176785165939, 44.0037646743764, 31.205732976832863, 24.37910615569483, 125.02111893866467, 45.85836885163255, 94.75678482792097, 86.76272659071134, 38.258737767412185, 58.069737000076636, 56.05941668404216, 103.56232234566423, 86.2397039600922, 51.64504497152984, 36.317116671479006, 29.475806906525474, 41.36363216184027, 88.7406455147794, 38.879043078353035, 52.64978643929391, 81.46303433789247, 32.351701283801845, 62.211075564875244, 50.76473301337671, 65.99496255966677, 41.17448098953305, 55.63294925942984, 46.45812170477141, 46.335701908375626, 38.642449066734635, 3.1934525548467882, 3.193451981128806, 3.193451407410824, 3.1934511205518326, 3.1932310997056668, 2.4330226451103543, 2.4330226451103543, 2.4330229319693455, 2.4330214976743902, 2.433020923956408, 2.433018055366497, 2.4330226451103543, 2.4330220713923723, 2.433012031327684, 6.236675908623816, 1.6725842730336835, 1.672584129604188, 1.6725849901811614, 1.6725848467516657, 1.6725842730336835, 1.672584129604188, 1.672583842745197, 1.6725845598926747, 1.6725836993157013, 1.6725835558862059, 1.672583842745197, 1.672582838738728, 1.6725844164631791, 1.6725848467516657, 1.672583842745197, 6.235580107277853, 3.9558179021961055, 3.1576568556436313, 6.216952631832437, 3.9545152755175663, 4.715339329507757, 8.152738491531368, 2.9356199644882763, 3.1948613193520305, 7.756937913959252, 7.159264337433476, 6.970540954631376, 10.745041312598351, 3.954278616849917, 4.715491077914044, 7.321603577671088, 8.489217777182049, 14.47638488279181, 21.52975866597233, 11.301953947644195, 5.381797676071024, 6.949597379691901, 4.669683139344875, 9.192766991293244, 8.260105795870452, 11.21097030772222, 8.04846065852632, 16.355874665489342, 13.241324971013768, 13.631643673266154, 9.614046384453966, 12.014473816124754, 13.169028473207355, 9.08695561012183, 9.891241096134074, 10.874173756027393, 9.958658696220029, 8.333983460435752, 9.300039058447286, 9.083912036226366, 7.1052028919726675, 6.976193224191812, 7.023672976961053, 7.027828416305981, 4.239206303960169, 3.555428193110118, 2.8713094588256243, 2.8707692069189514, 2.870526632832362, 2.1878637352647963, 2.187862558672622, 2.1878643235608832, 2.1878643235608832, 2.187860989883056, 2.1878629508700134, 2.187391333506838, 2.1873830973616184, 3.5556344889380016, 1.5040764077760467, 1.5040762116773512, 1.5040762116773512, 1.5040762116773512, 1.5040762116773512, 1.5040762116773512, 1.5040762116773512, 1.5040757214306117, 1.5040754272825683, 1.5040757214306117, 1.5040745448384376, 1.504075525331916, 1.5040739565423504, 1.5040730740982198, 1.5040734662956112, 1.5040725838514806, 3.5558878484528553, 2.8587695355312532, 2.818092194784678, 6.977495938428217, 2.8740238569715992, 2.18795903923091, 8.352427824271249, 2.189141318267327, 2.188271816650562, 9.99687461991475, 2.1334300710143803, 16.784887448234407, 10.698232586637536, 4.1788730104494105, 8.344994899309173, 4.925230921468239, 10.921023452409306, 16.225731627295676, 5.608010105562359, 11.002378526099848, 9.426265066357166, 7.105663692757917, 4.240521734010971, 5.930561868603725, 4.239618503418545, 7.954669073847145, 3.556302793292971, 8.6926119976358, 5.195728677931592, 7.168975333259072, 5.939161973002648, 5.727897788564529, 4.881306382419453, 5.664984228415658, 4.586426106329292, 5.1878898286694035, 4.40240120731704, 4.590778320781811, 4.402811053591067, 4.304875050783722, 4.240275041851772, 7.65872562950029, 3.8919555272108366, 2.636827753195015, 2.63682683357947, 2.63682560742541, 2.0089659107504967, 2.0089659107504967, 2.0089656042119817, 2.008964531327179, 2.008962232288316, 2.008963305173119, 2.008712096860053, 1.381100389843798, 1.3810997767667679, 1.3810997767667679, 1.3811000833052829, 1.3810985506127076, 1.3810994702282526, 1.3810991636897376, 1.3810957917660724, 1.38109778426642, 1.3810931861886946, 1.380791399020649, 1.380788946712529, 1.3804729055035345, 1.3804634028095686, 1.3800235200405029, 1.375426668469136, 2.012514860408239, 2.009436447371065, 7.66464856668767, 3.2651808236098665, 5.77705612115492, 3.2661139268496133, 3.249828148623181, 3.9041051812539895, 10.001216939796066, 2.6374233575297152, 3.945043399936041, 3.8339995163284653, 2.009745438194214, 2.0092290740656487, 3.2657328994754327, 3.672083113843797, 8.642247711121778, 5.583291899651369, 7.362599614750315, 3.907997607317821, 7.373693856686225, 5.137499068015778, 5.831620589907016, 5.578107107208178, 4.063938347990525, 3.7390485982213693, 3.9428032164682127, 4.4670315906367115, 3.5765482398655712, 4.938900427591713, 3.266220602252843, 3.5382523831830657, 3.267727239054208, 3.2825937439560584, 4.513185034466019, 3.8863302930459174, 3.886323967468908, 3.259473744318099, 2.63261930411595, 2.005760797471438, 2.0057609480804146, 2.005758688945768, 1.3788989774294451, 1.3788986762114923, 1.3788986762114923, 1.378898525602516, 1.378898525602516, 1.3788986762114923, 1.378898525602516, 1.378898525602516, 1.378897471339681, 1.378897019512752, 1.3788971701217283, 1.378897471339681, 1.378898224384563, 1.378898224384563, 1.378898224384563, 1.3788976219486575, 1.37889626646787, 1.37889626646787, 1.3788971701217283, 1.37889626646787, 1.3788961158588935, 1.378895965249917, 4.51485287827078, 6.397914500577912, 7.651311921618993, 2.6332193302779663, 3.8881101899291273, 3.887453836009934, 4.837301274840844, 2.006093040873398, 2.633153363546299, 8.90566179382689, 10.158859808583207, 2.6338073077218698, 2.6333238529075946, 5.141181391927306, 7.816307669891278, 3.26123556812414, 3.5862512453060837, 2.0070298287066666, 5.141555504624709, 4.714535379899988, 2.0063615766783367, 5.141914556424469, 3.2610476081215802, 4.514339301661223, 4.261436407256065, 5.368138275530455, 4.514452860829436, 4.7883882987902116, 3.886988153054875, 3.888054765825809, 3.260075879005785, 3.9782180344253, 3.884639255458789, 3.3660076062174213, 3.260941278184235], \"Total\": [161.0, 119.0, 117.0, 85.0, 130.0, 101.0, 32.0, 59.0, 46.0, 69.0, 48.0, 56.0, 109.0, 96.0, 137.0, 28.0, 15.0, 62.0, 21.0, 89.0, 37.0, 14.0, 69.0, 25.0, 22.0, 44.0, 56.0, 39.0, 39.0, 15.0, 22.126532508284523, 19.33536758099731, 18.404122370104325, 17.474504389233516, 15.612004927057589, 32.05897627655823, 13.751728491118797, 13.751506162173694, 12.820324513020218, 12.82012692548646, 28.390480766714823, 13.726081167595243, 11.891165570479341, 11.891163843925673, 11.889290149494526, 10.960603103370058, 10.029700129119787, 10.029532971687775, 10.028853768943595, 10.028613854885963, 19.962451461172545, 19.958312321808062, 9.099486747242535, 9.099485992552658, 9.099441391684039, 9.09713994152705, 19.950832487035154, 9.093086491214505, 8.168930565698949, 8.168927250547682, 82.19971393678526, 15.310472875211257, 48.58714370919456, 22.456902615172346, 24.2788626755653, 54.973450258359925, 56.85211621315551, 67.21595669863942, 96.48800751127318, 51.41991443905979, 35.693935418504786, 27.48463348025654, 161.1383979530325, 54.4780695903204, 120.3408083406616, 109.78398762410811, 44.935982991584055, 71.20198261766832, 69.22385424469692, 137.07311749329884, 117.09155899723672, 65.30787152049872, 43.937082515638764, 34.42388368199354, 51.64187033528656, 130.82910784848508, 48.17663092896125, 69.82775790126172, 119.85872078893382, 38.80838785655557, 89.11329447027562, 69.26565439369719, 101.20385921283061, 54.66526466024894, 85.94571551281098, 66.58117708237755, 69.53554208346011, 56.27638156779116, 3.767264847263241, 3.7672642911239422, 3.7672662572322326, 3.767266022231939, 3.7672551096265527, 3.006833562148321, 3.006833643452245, 3.0068348763278805, 3.0068331965133126, 3.006833327545306, 3.0068298138677747, 3.0068358140509455, 3.0068351282776202, 3.006824723586427, 7.737377937037705, 2.246394836277571, 2.246394842170186, 2.246396013815156, 2.2463959071318373, 2.246395276376355, 2.2463951098779362, 2.246394813327082, 2.2463958249375473, 2.2463948649140297, 2.246394754087806, 2.246395169223449, 2.2463938286453398, 2.2463960754944265, 2.2463966919181853, 2.246395507055936, 8.668421454807339, 5.455993184872383, 4.704045450410986, 10.857836279766618, 6.387088844219566, 8.078057823566619, 17.317810754089425, 4.75379131123555, 5.3221891898824065, 17.386786014269408, 15.680413404238639, 15.340182546501426, 28.721808969714907, 7.317758388020426, 9.59440284146153, 18.94174881047843, 23.77865541888246, 62.79029678144849, 130.82910784848508, 46.19380853527322, 12.623637585945827, 20.154439214102133, 9.949447032428404, 36.025987515179615, 30.37713596664679, 59.470926449577576, 31.737172428776333, 137.07311749329884, 89.11329447027562, 101.20385921283061, 48.39322810361976, 85.94571551281098, 117.09155899723672, 53.7306308895915, 69.82775790126172, 120.3408083406616, 96.48800751127318, 66.58117708237755, 109.78398762410811, 119.85872078893382, 41.336425247735534, 45.461070467201644, 51.33050813481724, 65.30787152049872, 4.828347459386535, 4.144569238967748, 3.4608090794513457, 3.460838632344084, 3.4608526801489963, 2.7770040775903415, 2.7770031358643577, 2.7770054268868742, 2.7770055043456976, 2.777001340202249, 2.7770046613962576, 2.7770363453541145, 2.7770375618569063, 4.904574527233362, 2.093216223056961, 2.093216134540447, 2.093216157232237, 2.0932161735984187, 2.0932162189240864, 2.0932162348364027, 2.0932163057298725, 2.093215799201528, 2.093215504697679, 2.0932159210216805, 2.0932143716273552, 2.0932157370748987, 2.093213640871644, 2.093212938581639, 2.093213759931712, 2.0932127322149543, 5.0731651785184715, 4.394197992628309, 4.408893070453864, 15.340182546501426, 5.148704888570315, 3.53700099512227, 28.721808969714907, 3.705282813331725, 3.7056154564629593, 46.19380853527322, 3.725419944088506, 119.85872078893382, 56.072882914894336, 11.51050765133866, 37.34780077365665, 16.032740831933182, 69.53554208346011, 161.1383979530325, 22.944001804036397, 130.82910784848508, 101.20385921283061, 54.66526466024894, 15.652529882822954, 39.59382754172914, 15.993473544534524, 89.11329447027562, 10.656723702278418, 137.07311749329884, 32.29159931991536, 96.48800751127318, 62.79029678144849, 56.27638156779116, 40.168213645599806, 85.94571551281098, 34.61896773168721, 66.58117708237755, 40.064612768618844, 71.20198261766832, 120.3408083406616, 48.17663092896125, 30.451458540537615, 8.260228275504064, 4.492968330377832, 3.2371504664833752, 3.237150243769517, 3.237151317833149, 2.6092891927063406, 2.609289200173729, 2.6092903614113045, 2.609289465390885, 2.6092870010988136, 2.6092887239053857, 2.6093267197656442, 1.9814238292905821, 1.981423008636004, 1.9814232365203064, 1.9814239413195254, 1.9814220725228946, 1.9814234510586848, 1.9814234642884547, 1.9814203846766931, 1.9814243985371842, 1.9814185337514274, 1.9814707437033938, 1.9814695187465834, 1.9815232136566159, 1.981527446791302, 1.9816006622260047, 1.9824908915043078, 3.5362730185413778, 3.5378176466344886, 14.339970331088244, 6.6549469226028215, 15.680146884556567, 8.041292223522886, 8.219660876188815, 13.192978395122111, 85.94571551281098, 7.337333105362376, 17.317810754089425, 16.96001018443177, 4.468450003225041, 4.468711153792906, 12.998276311831763, 17.28498688230804, 119.85872078893382, 44.621839268209314, 101.20385921283061, 22.770996740810375, 117.09155899723672, 59.470926449577576, 137.07311749329884, 130.82910784848508, 40.064612768618844, 28.721808969714907, 39.564758191394574, 69.26565439369719, 28.141520411559732, 120.3408083406616, 21.20854423901578, 53.7306308895915, 25.584555552559404, 161.1383979530325, 5.113714527161724, 4.486859295367104, 4.48685777774452, 3.860001972594204, 3.2331455411511953, 2.6062860314721306, 2.606286841430392, 2.606285344131835, 1.979423438899965, 1.9794232648434171, 1.9794233829535637, 1.9794232031826453, 1.9794232465855384, 1.9794234758144655, 1.979423262480416, 1.9794233804329269, 1.9794219351322715, 1.9794214786296955, 1.9794217091914548, 1.9794221799497942, 1.9794233270564674, 1.9794233376013017, 1.9794233582059704, 1.9794227743468165, 1.9794208708955812, 1.9794209058893883, 1.9794222877636036, 1.9794215511570052, 1.9794215830004216, 1.9794219437352418, 9.786482987949329, 21.20854423901578, 32.29159931991536, 6.197554783215635, 13.324036266323985, 13.945125164734607, 25.01124930013134, 4.294913189391566, 7.637185646181011, 117.09155899723672, 161.1383979530325, 8.170829726087444, 8.397134049306807, 39.91799100067783, 109.78398762410811, 16.32581403118772, 21.09747988580806, 5.05485162343777, 59.470926449577576, 48.17663092896125, 5.225668694255222, 69.26565439369719, 23.77865541888246, 66.58117708237755, 56.27638156779116, 120.3408083406616, 69.53554208346011, 101.20385921283061, 69.82775790126172, 71.20198261766832, 26.754936598908206, 130.82910784848508, 119.85872078893382, 89.11329447027562, 67.21595669863942], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.2552, -6.3936, -6.4445, -6.4978, -6.6146, -5.8967, -6.7462, -6.7462, -6.8194, -6.8196, -6.0257, -6.7535, -6.8978, -6.8978, -6.8986, -6.9834, -7.0771, -7.0771, -7.0772, -7.0773, -6.3935, -6.3943, -7.1801, -7.1801, -7.1801, -7.181, -6.3968, -7.1839, -7.2952, -7.2952, -5.0264, -6.6779, -5.5662, -6.3084, -6.2376, -5.4642, -5.4376, -5.2859, -4.9564, -5.5429, -5.8866, -6.1334, -4.4987, -5.5016, -4.7758, -4.864, -5.6828, -5.2655, -5.3007, -4.687, -4.87, -5.3828, -5.7349, -5.9436, -5.6048, -4.8414, -5.6667, -5.3635, -4.927, -5.8505, -5.1966, -5.4, -5.1376, -5.6093, -5.3084, -5.4886, -5.4912, -5.6728, -6.6485, -6.6485, -6.6485, -6.6485, -6.6485, -6.9204, -6.9204, -6.9204, -6.9204, -6.9204, -6.9204, -6.9204, -6.9204, -6.9205, -5.9791, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -7.2952, -5.9793, -6.4344, -6.6598, -5.9823, -6.4347, -6.2588, -5.7112, -6.7327, -6.648, -5.761, -5.8412, -5.8679, -5.4351, -6.4348, -6.2587, -5.8188, -5.6708, -5.1371, -4.7401, -5.3846, -6.1266, -5.8709, -6.2685, -5.5912, -5.6981, -5.3927, -5.7241, -5.015, -5.2262, -5.1972, -5.5464, -5.3235, -5.2317, -5.6027, -5.5179, -5.4232, -5.5111, -5.6892, -5.5796, -5.6031, -5.8488, -5.8671, -5.8603, -5.8597, -5.9848, -6.1607, -6.3744, -6.3746, -6.3747, -6.6463, -6.6463, -6.6463, -6.6463, -6.6463, -6.6463, -6.6465, -6.6465, -6.1607, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -7.021, -6.1606, -6.3788, -6.3931, -5.4865, -6.3735, -6.6462, -5.3067, -6.6457, -6.6461, -5.1269, -6.6715, -4.6087, -5.0591, -5.9992, -5.3075, -5.8348, -5.0385, -4.6426, -5.705, -5.0311, -5.1857, -5.4683, -5.9845, -5.6491, -5.9847, -5.3555, -6.1605, -5.2667, -5.7814, -5.4594, -5.6476, -5.6839, -5.8438, -5.6949, -5.9061, -5.7829, -5.9471, -5.9052, -5.947, -5.9695, -5.9846, -5.1469, -5.8239, -6.2132, -6.2132, -6.2132, -6.4852, -6.4852, -6.4852, -6.4852, -6.4852, -6.4852, -6.4853, -6.8599, -6.8599, -6.8599, -6.8599, -6.8599, -6.8599, -6.8599, -6.8599, -6.8599, -6.8599, -6.8601, -6.8601, -6.8604, -6.8604, -6.8607, -6.864, -6.4834, -6.4849, -5.1462, -5.9995, -5.4289, -5.9992, -6.0042, -5.8208, -4.8801, -6.213, -5.8103, -5.8389, -6.4848, -6.485, -5.9993, -5.882, -5.0261, -5.463, -5.1864, -5.8198, -5.1849, -5.5462, -5.4195, -5.4639, -5.7806, -5.864, -5.8109, -5.6861, -5.9084, -5.5856, -5.9992, -5.9192, -5.9987, -5.9942, -5.6583, -5.8078, -5.8078, -5.9837, -6.1973, -6.4693, -6.4693, -6.4693, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -6.844, -5.6579, -5.3093, -5.1304, -6.1971, -5.8074, -5.8075, -5.5889, -6.4691, -6.1971, -4.9786, -4.8469, -6.1968, -6.197, -5.528, -5.1091, -5.9832, -5.8882, -6.4686, -5.5279, -5.6146, -6.469, -5.5279, -5.9832, -5.658, -5.7157, -5.4848, -5.658, -5.5991, -5.8076, -5.8074, -5.9835, -5.7844, -5.8082, -5.9516, -5.9833], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4461, 0.4425, 0.441, 0.4395, 0.4354, 0.4338, 0.4307, 0.4306, 0.4276, 0.4274, 0.4263, 0.4253, 0.4244, 0.4244, 0.4238, 0.4204, 0.4154, 0.4153, 0.4153, 0.4152, 0.4107, 0.4101, 0.4097, 0.4097, 0.4097, 0.409, 0.408, 0.4066, 0.4025, 0.4025, 0.3624, 0.3915, 0.3485, 0.378, 0.3708, 0.3269, 0.3199, 0.3042, 0.2722, 0.3151, 0.3365, 0.351, 0.2171, 0.2986, 0.2318, 0.2355, 0.31, 0.267, 0.2599, 0.1905, 0.165, 0.2361, 0.2804, 0.3157, 0.2489, 0.0827, 0.2564, 0.1885, 0.0847, 0.2889, 0.1115, 0.1601, 0.0433, 0.1874, 0.0359, 0.111, 0.0649, 0.0949, 1.8232, 1.8232, 1.8232, 1.8232, 1.8231, 1.7767, 1.7767, 1.7767, 1.7767, 1.7767, 1.7767, 1.7767, 1.7767, 1.7767, 1.7728, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.6935, 1.659, 1.6669, 1.5898, 1.4308, 1.509, 1.4501, 1.2351, 1.5064, 1.4781, 1.1813, 1.2044, 1.1997, 1.0052, 1.3729, 1.2781, 1.0379, 0.9584, 0.5212, 0.184, 0.5806, 1.1359, 0.9237, 1.232, 0.6226, 0.6862, 0.3198, 0.6164, -0.1375, 0.0819, -0.0163, 0.3723, 0.0208, -0.1967, 0.2113, 0.0341, -0.4155, -0.2825, -0.0896, -0.4801, -0.5914, 0.2275, 0.1141, -0.0006, -0.2408, 2.2387, 2.2155, 2.1821, 2.1819, 2.1818, 2.1304, 2.1304, 2.1304, 2.1304, 2.1304, 2.1304, 2.1301, 2.1301, 2.0472, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0383, 2.0134, 1.9389, 1.9212, 1.581, 1.7858, 1.8885, 1.1337, 1.8426, 1.8421, 0.8382, 1.8114, 0.403, 0.7122, 1.3556, 0.8702, 1.1885, 0.5177, 0.0731, 0.9599, -0.107, -0.0048, 0.3285, 1.0629, 0.4703, 1.0411, -0.0473, 1.2713, -0.3892, 0.5418, -0.2308, 0.0106, 0.0839, 0.2611, -0.3506, 0.3475, -0.1833, 0.1605, -0.3727, -0.9393, -0.0463, 0.3973, 2.5396, 2.4716, 2.4101, 2.4101, 2.4101, 2.3538, 2.3538, 2.3538, 2.3538, 2.3538, 2.3538, 2.3536, 2.2543, 2.2543, 2.2543, 2.2543, 2.2543, 2.2543, 2.2543, 2.2543, 2.2543, 2.2543, 2.254, 2.254, 2.2538, 2.2538, 2.2534, 2.2496, 2.0515, 2.0496, 1.9888, 1.9032, 1.6167, 1.7142, 1.6873, 1.3976, 0.4642, 1.5921, 1.136, 1.1283, 1.8162, 1.8159, 1.2339, 1.0661, -0.0144, 0.5368, -0.0055, 0.8528, -0.1498, 0.1663, -0.542, -0.5398, 0.3269, 0.5764, 0.3092, -0.126, 0.5524, -0.578, 0.7445, -0.1051, 0.5573, -1.2784, 2.5078, 2.4891, 2.489, 2.4636, 2.4273, 2.3708, 2.3708, 2.3708, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 2.2712, 1.8591, 1.4343, 1.1928, 1.7768, 1.4011, 1.3554, 0.9898, 1.8715, 1.5679, 0.0565, -0.1312, 1.5006, 1.4731, 0.5832, -0.0096, 1.0221, 0.8607, 1.709, 0.1846, 0.3085, 1.6755, 0.0322, 0.646, -0.0584, 0.0521, -0.4771, -0.1018, -0.4182, -0.2557, -0.2749, 0.5278, -0.8603, -0.7965, -0.6434, -0.3932]}, \"token.table\": {\"Topic\": [5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 4, 5, 5, 1, 3, 4, 5, 1, 1, 2, 4, 1, 2, 3, 4, 5, 2, 2, 4, 1, 2, 5, 2, 1, 2, 4, 5, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 3, 1, 2, 1, 2, 5, 2, 4, 1, 3, 5, 1, 3, 1, 3, 1, 2, 2, 5, 4, 1, 2, 3, 5, 1, 3, 1, 5, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 1, 2, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 5, 4, 4, 1, 2, 3, 3, 1, 1, 2, 4, 5, 1, 3, 4, 1, 1, 4, 1, 4, 2, 1, 2, 3, 4, 5, 3, 1, 2, 4, 5, 3, 1, 2, 3, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 3, 1, 3, 1, 2, 3, 4, 5, 3, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 4, 1, 3, 1, 4, 5, 1, 2, 3, 4, 5, 5, 2, 4, 1, 3, 2, 2, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 5, 1, 1, 2, 3, 4, 5, 2, 2, 4, 1, 2, 3, 4, 2, 1, 2, 3, 5, 1, 1, 2, 1, 2, 3, 4, 5, 5, 4, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 4, 3, 3, 5, 1, 3, 1, 2, 3, 4, 5, 4, 1, 2, 4, 5, 5, 1, 3, 5, 1, 2, 5, 5, 2, 4, 1, 2, 3, 5, 1, 5, 5, 3, 2, 1, 2, 3, 4, 5, 3, 1, 1, 2, 3, 4, 5, 5, 1, 5, 1, 2, 3, 5, 1, 2, 1, 2, 4, 1, 3, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 2, 1, 2, 3, 4, 5, 5, 1, 2, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 5, 1, 4, 3, 4, 1, 2, 3, 4, 5, 4, 5, 1, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 5, 1, 2, 2, 1, 2, 3, 5, 1, 2, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 1, 3, 5, 1, 2, 3, 1, 3, 4, 5, 2, 3, 3, 1, 5, 1, 2, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 4, 5, 5, 3, 1, 1, 4, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 1, 3, 1, 2, 3, 4, 2, 1, 2, 3, 1, 3, 4, 5, 5, 1, 3, 1, 2, 3, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 4, 5, 3, 1, 2, 3, 4, 5, 4, 1, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 5, 1, 3, 4, 3, 1, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 1, 5, 1, 2, 4, 5, 1, 2, 3, 1, 4, 2, 3, 1, 2, 3, 4, 5, 2, 4, 3, 1, 2, 3, 5, 1, 3, 1, 2, 4, 5, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3], \"Freq\": [0.5051979985930471, 0.6896221398408343, 0.0328391495162302, 0.1313565980649208, 0.09851744854869061, 0.0328391495162302, 0.5051976638775856, 0.6819021127618469, 0.15397789643009444, 0.06599052704146904, 0.043993684694312696, 0.06599052704146904, 0.7939294168434671, 0.11618479270880006, 0.038728264236266685, 0.019364132118133343, 0.019364132118133343, 0.7202005989618271, 0.7664929151748225, 0.2826601311549497, 0.5653202623098994, 0.6957435517175335, 0.14588171245690218, 0.0897733615119398, 0.02244334037798495, 0.033665010566977425, 0.20920545375858762, 0.06973515125286255, 0.5578812100229004, 0.20920545375858762, 0.7664919280651642, 0.9278889433884929, 0.5051981151039521, 0.8732152101369767, 0.036383967089040695, 0.036383967089040695, 0.036383967089040695, 0.9360281742721249, 0.6942440906497036, 0.11570734844161726, 0.23141469688323452, 0.6239920536454453, 0.12479841072908905, 0.09983872858327124, 0.09983872858327124, 0.04991936429163562, 0.6651515305328743, 0.7963338295824652, 0.5046875813328814, 0.9780417472793606, 0.6651535037314963, 0.5051980884659208, 0.8903149457113455, 0.37526166246164594, 0.15010466498465835, 0.15010466498465835, 0.3002093299693167, 0.7963334636566579, 0.17375428265906256, 0.17375428265906256, 0.34750856531812513, 0.26063142398859385, 0.873148761345947, 0.03638119838941446, 0.05457179758412169, 0.03638119838941446, 0.01819059919470723, 0.9554677936942082, 0.21258298002044845, 0.6377489400613453, 0.18789260665536298, 0.563677819966089, 0.18789260665536298, 0.665151257804524, 0.7664921912756774, 0.8222781585996267, 0.03737627993634667, 0.11212883980904001, 0.9890668557944832, 0.72020130888894, 0.656862296101699, 0.37534988348668513, 0.3713763958519725, 0.6189606597532875, 0.8903150450146525, 0.5051982702130214, 0.7664922713781721, 0.7966498157556333, 0.04979061348472708, 0.1244765337118177, 0.02489530674236354, 0.2684261143731727, 0.5368522287463454, 0.9523412124454935, 0.05012322170765755, 0.890315425501358, 0.7072315990125703, 0.16750222081876665, 0.0372227157375037, 0.0744454314750074, 0.01861135786875185, 0.1832846864201848, 0.7331387456807392, 0.4099616085864717, 0.5466154781152956, 0.37307436623484647, 0.12435812207828216, 0.37307436623484647, 0.12435812207828216, 0.6651510503679645, 0.5829125430954977, 0.24981966132664188, 0.027757740147404654, 0.08327322044221395, 0.05551548029480931, 0.9793203572560303, 0.6651523776888923, 0.5051976406481176, 0.5044159366811533, 0.5046880287987936, 0.5176344836022979, 0.46011954097982033, 0.05751494262247754, 0.9554674604156755, 0.9893219251158684, 0.5736771743168682, 0.07170964678960852, 0.07170964678960852, 0.2868385871584341, 0.26986070512413834, 0.5397214102482767, 0.9684962368079127, 0.8974106320973937, 0.28278359582441814, 0.5655671916488363, 0.4475563380959319, 0.4475563380959319, 0.8903153048941852, 0.8443764682912558, 0.03671202036048938, 0.07342404072097876, 0.03671202036048938, 0.01835601018024469, 0.7202008431933185, 0.36715977453571697, 0.24477318302381132, 0.12238659151190566, 0.36715977453571697, 0.9554690592215536, 0.23097607756543018, 0.46195215513086035, 0.057744019391357544, 0.23097607756543018, 0.925056499701617, 0.5046877903615302, 0.8089696913154764, 0.10112121141443454, 0.028891774689838442, 0.028891774689838442, 0.028891774689838442, 0.8193534467652995, 0.0682794538971083, 0.0682794538971083, 0.022759817965702766, 0.022759817965702766, 0.5051976353892982, 0.720200249029428, 0.19711560038185716, 0.7884624015274286, 0.7077005213718218, 0.10110007448168884, 0.07582505586126663, 0.10110007448168884, 0.02527501862042221, 0.9554674883683074, 0.8668419185924452, 0.6485845161872198, 0.11792445748858542, 0.05896222874429271, 0.23584891497717084, 0.3969348844200225, 0.3473180238675197, 0.09923372110500563, 0.14885058165750845, 0.04961686055250281, 0.44758249472558226, 0.44758249472558226, 0.22681430100935907, 0.6804429030280772, 0.3065452628583809, 0.20436350857225394, 0.5109087714306348, 0.7757306860927967, 0.03723507293245424, 0.09929352781987798, 0.01861753646622712, 0.06205845488742373, 0.5051979361094984, 0.8903151710008291, 0.5046875527980408, 0.9728459028843262, 0.955467603659262, 0.890315836206724, 0.6651515485183825, 0.7924652937355607, 0.0819791683174718, 0.02732638943915727, 0.02732638943915727, 0.07287037183775272, 0.5279343581236848, 0.36955405068657937, 0.05279343581236848, 0.6354897745799364, 0.1444294942227128, 0.1444294942227128, 0.05777179768908513, 0.028885898844542564, 0.7362956496465402, 0.0866230176054753, 0.04331150880273765, 0.05774867840365021, 0.07218584800456276, 0.6930083085214027, 0.12438610665768766, 0.10661666284944657, 0.017769443808241095, 0.07107777523296438, 0.8187546000550382, 0.103639822791777, 0.07254787595424389, 0.8480129243054245, 0.07438709862328285, 0.02975483944931314, 0.044632259173969714, 0.9453357087725894, 0.6693834571816021, 0.05355067657452816, 0.21420270629811264, 0.02677533828726408, 0.02677533828726408, 0.8903150124385546, 0.8903154694252037, 0.5046876776721779, 0.7817216116540769, 0.039086080582703846, 0.039086080582703846, 0.11725824174811154, 0.7963357703952758, 0.420545225280445, 0.336436180224356, 0.084109045056089, 0.12616356758413352, 0.8974321008097426, 0.5025404913160916, 0.5025404913160916, 0.5892630214630825, 0.22296438649954473, 0.09555616564266202, 0.03185205521422067, 0.04777808282133101, 0.5051976290731024, 0.9267410450824901, 0.32270791787374714, 0.16135395893687357, 0.48406187681062074, 0.7962286748800002, 0.10718462931076926, 0.03062417980307693, 0.03062417980307693, 0.03062417980307693, 0.33005565686694427, 0.18860323249539673, 0.14145242437154754, 0.28290484874309507, 0.7232416883837108, 0.20664048239534594, 0.04132809647906919, 0.020664048239534594, 0.020664048239534594, 0.5051976147942024, 0.3826429728171653, 0.44641680162002617, 0.06377382880286088, 0.06377382880286088, 0.06377382880286088, 0.914406766799933, 0.06531476905713807, 0.9554674657925082, 0.9554685901096154, 0.7673752436636455, 0.9607990818657182, 0.8284407933865264, 0.7587191558919953, 0.1167260239833839, 0.06565838849065345, 0.043772258993768964, 0.021886129496884482, 0.5046889301609065, 0.4287688189163296, 0.3062634420830926, 0.06125268841661852, 0.18375806524985555, 0.5051979085927252, 0.9510229933004505, 0.035223073825942613, 0.5051976598208205, 0.38272613841720904, 0.19136306920860452, 0.38272613841720904, 0.5051980965931603, 0.8903147014039677, 0.926740737596446, 0.4762543011191852, 0.2381271505595926, 0.21647922778144782, 0.06494376833443434, 0.9669678698588838, 0.031192511930931737, 0.5051976749550753, 0.95546896502331, 0.8903153284146328, 0.6763867450028115, 0.15030816555618035, 0.05010272185206011, 0.025051360926030056, 0.1252568046301503, 0.720200228940932, 0.8973349037494949, 0.6908859532940745, 0.12015407883375209, 0.07509629927109505, 0.030038519708438023, 0.07509629927109505, 0.7673754821416601, 0.9123585541497529, 0.5051980562588028, 0.4763530005014276, 0.1190882501253569, 0.1190882501253569, 0.3572647503760707, 0.3131317019036047, 0.6262634038072094, 0.1292427496934259, 0.7754564981605553, 0.9267409813233057, 0.2275728134411778, 0.6827184403235333, 0.6420215642316746, 0.08916966169884369, 0.19617325573745611, 0.05350179701930621, 0.035667864679537475, 0.90613799723581, 0.08237618156689182, 0.5749869233520248, 0.12777487185600553, 0.25554974371201106, 0.7963334139817517, 0.675158972299, 0.0710693655051579, 0.0710693655051579, 0.1421387310103158, 0.03553468275257895, 0.5051976053727129, 0.1978297434811562, 0.3956594869623124, 0.3956594869623124, 0.8903154345154434, 0.6802767477637475, 0.16815829720002748, 0.08407914860001374, 0.045861353781825674, 0.03057423585455045, 0.6615321980921418, 0.07190567370566758, 0.1581924821524687, 0.04314340422340055, 0.07190567370566758, 0.8145840588659068, 0.04213365821720207, 0.07022276369533678, 0.028089105478134715, 0.05617821095626943, 0.5046759440602292, 0.561351305702782, 0.12474473460061823, 0.3118618365015456, 0.062372367300309116, 0.573974215054333, 0.3826494767028886, 0.9554676592651911, 0.5046877323171796, 0.8556995957693885, 0.05834315425700376, 0.01944771808566792, 0.01944771808566792, 0.03889543617133584, 0.7664924090909156, 0.505198261281717, 0.4088679029452127, 0.13628930098173755, 0.4088679029452127, 0.13628930098173755, 0.5237531448512329, 0.13093828621280823, 0.3928148586384247, 0.1044502455664713, 0.3829842337437281, 0.2785339881772568, 0.1392669940886284, 0.1044502455664713, 0.6757956322814224, 0.07508840358682471, 0.1418336512195578, 0.07508840358682471, 0.03337262381636654, 0.9554674280557119, 0.8914921856654142, 0.9517869103880083, 0.050094047915158334, 0.9890717037009636, 0.8903154459468535, 0.6651512020699462, 0.7196761658725092, 0.0399820092151394, 0.0399820092151394, 0.19991004607569698, 0.23283357681594116, 0.23283357681594116, 0.4656671536318823, 0.9554686444557906, 0.5046611903455112, 0.861885243045033, 0.07035797902408433, 0.017589494756021083, 0.035178989512042166, 0.017589494756021083, 0.6923994985248693, 0.07693327761387436, 0.2307998328416231, 0.505197996397354, 0.21035841384885945, 0.6310752415465783, 0.8095211152790524, 0.08302780669528742, 0.10378475836910928, 0.23072251510000574, 0.6921675453000172, 0.9554675061966726, 0.7829503214188666, 0.15153877188752254, 0.02525646198125376, 0.02525646198125376, 0.890315436850868, 0.9554674676789962, 0.9554674958387994, 0.9519843007586902, 0.05010443688203633, 0.2762981428989168, 0.5525962857978336, 0.0920993809663056, 0.0920993809663056, 0.8902800344607867, 0.5356494828942597, 0.25207034489141633, 0.06301758622285408, 0.06301758622285408, 0.06301758622285408, 0.7590104793990864, 0.14320952441492196, 0.028641904882984393, 0.028641904882984393, 0.057283809765968786, 0.7772016753617824, 0.8914924872013089, 0.6499059759883308, 0.13446330537689602, 0.06723165268844801, 0.13446330537689602, 0.02241055089614934, 0.5051976592177244, 0.9554676876234648, 0.8973498592014175, 0.9250566340164504, 0.5046876743024279, 0.20389128444217838, 0.8155651377687135, 0.7664812477678645, 0.8850077760768379, 0.08232630475133376, 0.02058157618783344, 0.6515740740055006, 0.13962301585832154, 0.06981150792916077, 0.11635251321526796, 0.034905753964580385, 0.866849320817085, 0.5046425443139807, 0.41690974061608965, 0.5211371757701121, 0.10422743515402241, 0.6877805480698124, 0.25010201747993177, 0.8684948755728595, 0.056031927456313524, 0.028015963728156762, 0.028015963728156762, 0.8903149701567131, 0.06518827249732213, 0.456317907481255, 0.456317907481255, 0.5305852697059017, 0.07579789567227167, 0.3031915826890867, 0.07579789567227167, 0.7673756845170798, 0.9360137481553527, 0.7201922305935717, 0.9471020782458006, 0.2827253940213922, 0.5654507880427844, 0.5596314286727196, 0.263355966434221, 0.06583899160855525, 0.09875848741283288, 0.032919495804277624, 0.26988493196847707, 0.5397698639369541, 0.5046622684549044, 0.23764940807077398, 0.3960823467846233, 0.23764940807077398, 0.07921646935692465, 0.8668384000300309, 0.6148171799133009, 0.08783102570190013, 0.08783102570190013, 0.17566205140380026, 0.043915512850950064, 0.7664922691845879, 0.9252024184528516, 0.5046756320666069, 0.9002464419390662, 0.060827462293180146, 0.02433098491727206, 0.01216549245863603, 0.9942814126778723, 0.7403004836849605, 0.1363711417314401, 0.03896318335184003, 0.03896318335184003, 0.03896318335184003, 0.9777628323681887, 0.8905947691329252, 0.04452973845664626, 0.04452973845664626, 0.9651183921338577, 0.9826552259949323, 0.6651516004156918, 0.8245640122511431, 0.07730287614854467, 0.02576762538284822, 0.02576762538284822, 0.02576762538284822, 0.505197629716431, 0.7344679730673801, 0.11102422848692954, 0.00854032526822535, 0.059782276877577444, 0.07686292741402814, 0.9897629378864434, 0.9793207546882785, 0.5051976433394193, 0.6389676816657346, 0.18496432890323897, 0.08407469495601771, 0.08407469495601771, 0.19422360023389856, 0.19422360023389856, 0.5826708007016956, 0.4507924758664593, 0.4507924758664593, 0.665151629401716, 0.7201919151077925, 0.7894246458032202, 0.09140706425089919, 0.03323893245487244, 0.04154866556859054, 0.04154866556859054, 0.8903152624262043, 0.5046874363403745, 0.7202004475550339, 0.5574205173820167, 0.03096780652122315, 0.15483903260611576, 0.2477424521697852, 0.7409343908353988, 0.2615062555889643, 0.7499439009109338, 0.16934217117343667, 0.07257521621718714, 0.02419173873906238, 0.5046884587104766, 0.5051977844045908, 0.9453509925886617, 0.8456474626830112, 0.04450776119384269, 0.022253880596921346, 0.04450776119384269, 0.022253880596921346, 0.750019235337464, 0.054879456244204684, 0.12805206456981091, 0.01829315208140156, 0.03658630416280312, 0.989066773763621, 0.36497856118256317, 0.12165952039418772, 0.36497856118256317, 0.12165952039418772, 0.7963337120243547, 0.8424383566915588, 0.029049598506605472, 0.058099197013210944, 0.058099197013210944, 0.6521490436565539, 0.13833464562411749, 0.08892941504407553, 0.06916732281205874, 0.04940523058004196, 0.6161873394530355, 0.1421970783353159, 0.047399026111771964, 0.047399026111771964, 0.18959610444708785, 0.9554683108950345], \"Term\": [\"afford\", \"again\", \"again\", \"again\", \"again\", \"again\", \"agent\", \"another\", \"another\", \"another\", \"another\", \"another\", \"asked\", \"asked\", \"asked\", \"asked\", \"asked\", \"assorted\", \"attendant\", \"attentive\", \"attentive\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bag\", \"bag\", \"bag\", \"bag\", \"bagging\", \"baklava\", \"band\", \"bar\", \"bar\", \"bar\", \"bar\", \"beef\", \"beer\", \"beer\", \"beer\", \"better\", \"better\", \"better\", \"better\", \"better\", \"blanco\", \"body\", \"bosa\", \"bread\", \"bride\", \"broken\", \"bubble\", \"business\", \"business\", \"business\", \"business\", \"butai\", \"buy\", \"buy\", \"buy\", \"buy\", \"came\", \"came\", \"came\", \"came\", \"came\", \"can\", \"certain\", \"certain\", \"certainly\", \"certainly\", \"certainly\", \"charcoal\", \"checkout\", \"chicken\", \"chicken\", \"chicken\", \"child\", \"chinese\", \"chip\", \"chip\", \"chocolate\", \"chocolate\", \"circle\", \"city\", \"co\", \"come\", \"come\", \"come\", \"come\", \"commission\", \"commission\", \"completely\", \"completely\", \"costco\", \"could\", \"could\", \"could\", \"could\", \"could\", \"cousin\", \"cousin\", \"cream\", \"cream\", \"credit\", \"credit\", \"credit\", \"credit\", \"crystal\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"dad\", \"data\", \"delta\", \"denny\", \"denver\", \"dessert\", \"dessert\", \"dessert\", \"dick\", \"didnt\", \"dish\", \"dish\", \"dish\", \"dish\", \"dollar\", \"dollar\", \"donut\", \"down\", \"dozen\", \"dozen\", \"dr\", \"dr\", \"drastically\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"duck\", \"early\", \"early\", \"early\", \"early\", \"effort\", \"employee\", \"employee\", \"employee\", \"employee\", \"entree\", \"estimate\", \"even\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"everybody\", \"exceeded\", \"expectation\", \"expectation\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"facial\", \"fail\", \"feel\", \"feel\", \"feel\", \"feel\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fit\", \"fit\", \"fitness\", \"fitness\", \"flight\", \"flight\", \"flight\", \"food\", \"food\", \"food\", \"food\", \"food\", \"fraction\", \"freezer\", \"fri\", \"fry\", \"funny\", \"furthermore\", \"gallo\", \"get\", \"get\", \"get\", \"get\", \"get\", \"getting\", \"getting\", \"getting\", \"give\", \"give\", \"give\", \"give\", \"give\", \"go\", \"go\", \"go\", \"go\", \"go\", \"going\", \"going\", \"going\", \"going\", \"going\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"got\", \"group\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"h\", \"hardwood\", \"hopefully\", \"horrible\", \"horrible\", \"horrible\", \"horrible\", \"host\", \"hour\", \"hour\", \"hour\", \"hour\", \"hungry\", \"ice\", \"ice\", \"im\", \"im\", \"im\", \"im\", \"im\", \"inform\", \"insurance\", \"internet\", \"internet\", \"internet\", \"it\", \"it\", \"it\", \"it\", \"it\", \"item\", \"item\", \"item\", \"item\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"jungle\", \"kept\", \"kept\", \"kept\", \"kept\", \"kept\", \"kid\", \"kid\", \"l\", \"lard\", \"lax\", \"leave\", \"lee\", \"like\", \"like\", \"like\", \"like\", \"like\", \"limited\", \"line\", \"line\", \"line\", \"line\", \"listed\", \"little\", \"little\", \"luggage\", \"machine\", \"machine\", \"machine\", \"maintenance\", \"male\", \"matthew\", \"me\", \"me\", \"me\", \"me\", \"meal\", \"meal\", \"mechanical\", \"medium\", \"melted\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"mimi\", \"mind\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"missed\", \"mom\", \"monday\", \"morning\", \"morning\", \"morning\", \"morning\", \"mother\", \"mother\", \"mousse\", \"mousse\", \"movie\", \"music\", \"music\", \"never\", \"never\", \"never\", \"never\", \"never\", \"nice\", \"nice\", \"no\", \"no\", \"no\", \"nobuo\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"obligatory\", \"offered\", \"offered\", \"offered\", \"oj\", \"one\", \"one\", \"one\", \"one\", \"one\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ordered\", \"ordered\", \"ordered\", \"ordered\", \"ordered\", \"otherwise\", \"outside\", \"outside\", \"outside\", \"outside\", \"pasta\", \"pasta\", \"patronize\", \"penne\", \"people\", \"people\", \"people\", \"people\", \"people\", \"personal\", \"pho\", \"phoenix\", \"phoenix\", \"phoenix\", \"phoenix\", \"pita\", \"pita\", \"pita\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"place\", \"place\", \"place\", \"place\", \"place\", \"placing\", \"plane\", \"plate\", \"plate\", \"polish\", \"prescription\", \"presentation\", \"price\", \"price\", \"price\", \"price\", \"promised\", \"promised\", \"promised\", \"r\", \"rb\", \"really\", \"really\", \"really\", \"really\", \"really\", \"received\", \"received\", \"received\", \"record\", \"regular\", \"regular\", \"restaurant\", \"restaurant\", \"restaurant\", \"rib\", \"rib\", \"richardson\", \"right\", \"right\", \"right\", \"right\", \"robson\", \"rockerij\", \"rokerij\", \"roll\", \"roll\", \"room\", \"room\", \"room\", \"room\", \"root\", \"rude\", \"rude\", \"rude\", \"rude\", \"rude\", \"said\", \"said\", \"said\", \"said\", \"said\", \"sampler\", \"sapporo\", \"say\", \"say\", \"say\", \"say\", \"say\", \"screwing\", \"seasonal\", \"seat\", \"seated\", \"sec\", \"sell\", \"sell\", \"serrano\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"session\", \"settle\", \"showed\", \"showed\", \"showed\", \"shrimp\", \"shrimp\", \"since\", \"since\", \"since\", \"since\", \"slap\", \"slice\", \"slice\", \"slice\", \"small\", \"small\", \"small\", \"small\", \"smoothie\", \"son\", \"sore\", \"spicy\", \"stale\", \"stale\", \"star\", \"star\", \"star\", \"star\", \"star\", \"status\", \"status\", \"stevia\", \"store\", \"store\", \"store\", \"store\", \"sucked\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"surgery\", \"sweet\", \"syrup\", \"table\", \"table\", \"table\", \"table\", \"taco\", \"take\", \"take\", \"take\", \"take\", \"take\", \"takeout\", \"tasted\", \"tasted\", \"tasted\", \"tempura\", \"thai\", \"thats\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"ticketing\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tiny\", \"today\", \"tokyo\", \"told\", \"told\", \"told\", \"told\", \"topping\", \"topping\", \"topping\", \"total\", \"total\", \"towel\", \"training\", \"u\", \"u\", \"u\", \"u\", \"u\", \"uncommon\", \"viewing\", \"visiting\", \"wait\", \"wait\", \"wait\", \"wait\", \"waiter\", \"waiter\", \"wanted\", \"wanted\", \"wanted\", \"wanted\", \"wear\", \"weekly\", \"weird\", \"well\", \"well\", \"well\", \"well\", \"well\", \"went\", \"went\", \"went\", \"went\", \"went\", \"wife\", \"wing\", \"wing\", \"wing\", \"wing\", \"wm\", \"worst\", \"worst\", \"worst\", \"worst\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"yakisoba\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 1, 3, 2, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2960020730069778725960769360\", ldavis_el2960020730069778725960769360_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2960020730069778725960769360\", ldavis_el2960020730069778725960769360_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2960020730069778725960769360\", ldavis_el2960020730069778725960769360_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4     -0.003069 -0.064179       1        1  62.446555\n",
       "0      0.073057 -0.060218       2        1  13.690966\n",
       "2     -0.101707 -0.014993       3        1   9.359234\n",
       "1      0.049144  0.076708       4        1   7.315107\n",
       "3     -0.017424  0.062681       5        1   7.188139, topic_info=        Term        Freq       Total Category  logprob  loglift\n",
       "41      food  161.000000  161.000000  Default  30.0000  30.0000\n",
       "80     place  119.000000  119.000000  Default  29.0000  29.0000\n",
       "107     time  117.000000  117.000000  Default  28.0000  28.0000\n",
       "422  service   85.000000   85.000000  Default  27.0000  27.0000\n",
       "73       one  130.000000  130.000000  Default  26.0000  26.0000\n",
       "..       ...         ...         ...      ...      ...      ...\n",
       "522  chicken    3.260076   26.754937   Topic5  -5.9835   0.5278\n",
       "73       one    3.978218  130.829108   Topic5  -5.7844  -0.8603\n",
       "80     place    3.884639  119.858721   Topic5  -5.8082  -0.7965\n",
       "8       back    3.366008   89.113294   Topic5  -5.9516  -0.6434\n",
       "48       got    3.260941   67.215957   Topic5  -5.9833  -0.3932\n",
       "\n",
       "[370 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "464       5  0.505198    afford\n",
       "0         1  0.689622     again\n",
       "0         2  0.032839     again\n",
       "0         3  0.131357     again\n",
       "0         4  0.098517     again\n",
       "...     ...       ...       ...\n",
       "124       2  0.142197     wrong\n",
       "124       3  0.047399     wrong\n",
       "124       4  0.047399     wrong\n",
       "124       5  0.189596     wrong\n",
       "3043      3  0.955468  yakisoba\n",
       "\n",
       "[636 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 1, 3, 2, 4])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pyLDAvis.gensim_models.prepare(lda, c, d)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
